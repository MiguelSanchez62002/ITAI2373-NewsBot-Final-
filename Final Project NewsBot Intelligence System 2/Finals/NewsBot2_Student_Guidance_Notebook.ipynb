{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPPilUbTS1_V"
      },
      "source": [
        "# ü§ñ NewsBot 2.0 Final Project - Student Guidance Notebook## üéØ Your Mission: Build an Advanced NLP Intelligence SystemWelcome to your final project! This notebook will guide you through building NewsBot 2.0 - a sophisticated news analysis platform that demonstrates everything you've learned in this course.### üöÄ What You're BuildingYou're creating a **production-ready news intelligence system** that can:- **Analyze** news articles with advanced NLP techniques- **Discover** hidden topics and trends in large text collections- **Understand** multiple languages and cultural contexts  - **Converse** with users through natural language queries- **Generate** insights and summaries automatically### üìö Skills You'll DemonstrateThis project integrates **ALL course modules**:- **Modules 1-2**: Advanced text preprocessing and feature engineering- **Modules 3-4**: Enhanced classification and linguistic analysis- **Modules 5-6**: Syntax parsing and semantic understanding- **Modules 7-8**: Multi-class classification and entity recognition- **Module 9**: Topic modeling and unsupervised learning- **Module 10**: Neural networks and language models- **Module 11**: Machine translation and multilingual processing- **Module 12**: Conversational AI and natural language understanding---## üó∫Ô∏è Project RoadmapThis notebook is organized into **7 major sections** that mirror your final system architecture:1. **üèóÔ∏è Project Setup & Architecture Planning**2. **üìä Advanced Content Analysis Engine** 3. **üß† Language Understanding & Generation**4. **üåç Multilingual Intelligence**5. **üí¨ Conversational Interface**6. **üîß System Integration & Testing**7. **üìà Evaluation & Documentation**Each section provides:- **Clear objectives** and success criteria- **Implementation hints** and architectural guidance- **Code templates** with TODO sections for you to complete- **Testing strategies** to validate your work- **Reflection questions** to deepen your understanding---## ‚ö†Ô∏è Important Notes### üéØ Learning Goals- **Understand** how advanced NLP systems work in production- **Implement** sophisticated text analysis pipelines- **Integrate** multiple NLP techniques into cohesive workflows- **Evaluate** system performance using appropriate metrics- **Communicate** technical concepts to business stakeholders### üö´ What This Notebook Won't Do- **Give you the answers** - you need to implement the logic- **Write your code** - you'll build everything from scratch- **Make decisions** - you'll choose the best approaches for your use case### ‚úÖ What This Notebook Will Do- **Guide your thinking** with structured questions and prompts- **Provide templates** and architectural patterns- **Suggest resources** and implementation strategies- **Help you organize** your work effectively- **Connect concepts** from different course modulesLet's begin building your NewsBot 2.0! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZQd4NsMS1_W"
      },
      "source": [
        "## üèóÔ∏è Section 1: Project Setup & Architecture PlanningBefore you start coding, you need to plan your system architecture and set up your development environment.### üéØ Section Objectives- Set up a professional development environment- Design your system architecture- Plan your data pipeline- Establish your project structure\n",
        "### ü§î Reflection Questions\n",
        "**1.**What are the main components your NewsBot 2.0 needs?\n",
        "Data ingestion (API/CSV)\n",
        "Preprocessing pipeline\n",
        "TF-IDF + ML model\n",
        "Sentiment/emotion analysis\n",
        "NER extraction\n",
        "Final output/visualization module\n",
        "\n",
        "**2.**How will data flow through your system? Raw news ‚Üí Cleaned ‚Üí Features extracted ‚Üí Analyzed (sentiment, NER) ‚Üí Classified ‚Üí Displayed/stored\n",
        "\n",
        "**3.**What external APIs or services might you need? NewsAPI or ContextualWeb for real-time data\n",
        "\n",
        "OpenAI or HuggingFace transformers (for better summaries/classification)\n",
        "\n",
        "Google Translate API (if multilingual)\n",
        "\n",
        "GitHub (for version control)\n",
        "\n",
        "Optional: Flask/Streamlit for interface\n",
        "\n",
        "\n",
        "**4.**How will you handle errors and edge cases?\n",
        "try-except for API requests and file loading\n",
        "\n",
        "Check if text is missing or malformed\n",
        "\n",
        "Validate dataset columns before processing\n",
        "\n",
        "Log all errors to a .log file or terminal output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNOAE1aXS1_W",
        "outputId": "5b06e27e-a787-4acc-a827-7525a235276e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Environment setup complete!\n",
            "üéØ Ready to build NewsBot 2.0!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# üì¶ Environment Setup and Imports\n",
        "# ‚úÖ Import all libraries needed for NewsBot 2.0\n",
        "\n",
        "# Standard libraries\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, Counter\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP libraries - Text preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import spacy\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Deep learning / embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Semantic similarity and clustering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Web scraping / APIs\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Graph/network analysis (for entity relationships)\n",
        "import networkx as nx\n",
        "\n",
        "# Language detection and translation\n",
        "!pip install -q langdetect deep_translator rouge_score\n",
        "from langdetect import detect_langs\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Summarization scoring\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "print(\"‚úÖ Environment setup complete!\")\n",
        "print(\"üéØ Ready to build NewsBot 2.0!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl7ZcGGBpzXa"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "629c6fb9"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioydT3T0VkSh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--VkW07SS1_X"
      },
      "source": [
        "### üèóÔ∏è System Architecture DesignYour NewsBot 2.0 should have a **modular architecture** where each component has a specific responsibility.**Think about these questions:**- How will you organize your code into modules?- What classes and functions will you need?- How will components communicate with each other?- Where will you store configuration and settings?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I41IZ977S1_X"
      },
      "outputs": [],
      "source": [
        "# üèóÔ∏è Architecture Planning# TODO: Design your system architectureclass NewsBot2Config:    \"\"\"    Configuration management for NewsBot 2.0    TODO: Define all your system settings here    \"\"\"    def __init__(self):        # TODO: Add configuration parameters        # Hint: Consider settings for:        # - API keys and endpoints        # - Model parameters        # - File paths and directories        # - Processing limits and thresholds        passclass NewsBot2System:    \"\"\"    Main system orchestrator for NewsBot 2.0    TODO: This will be your main system class    \"\"\"    def __init__(self, config):        self.config = config        # TODO: Initialize all your system components        # Hint: You'll need components for:        # - Data processing        # - Classification        # - Topic modeling        # - Language models        # - Multilingual processing        # - Conversational interface            def analyze_article(self, article_text):        \"\"\"        TODO: Implement comprehensive article analysis        This should return all the insights your system can generate        \"\"\"        pass        def process_query(self, user_query):        \"\"\"        TODO: Handle natural language queries from users        \"\"\"        pass        def generate_insights(self, articles):        \"\"\"        TODO: Generate high-level insights from multiple articles        \"\"\"        pass# TODO: Initialize your system# config = NewsBot2Config()# newsbot = NewsBot2System(config)print(\"üèóÔ∏è System architecture planned!\")print(\"üí° Next: Start implementing individual components\")\n",
        "# üèóÔ∏è Architecture Planning\n",
        "\n",
        "\n",
        "class NewsBot2Config:\n",
        "    \"\"\"Configuration management for NewsBot 2.0\"\"\"\n",
        "    def __init__(self):\n",
        "        self.train_file_path = \"/mnt/data/BBC News Train 2.csv\"\n",
        "        self.test_file_path = \"/mnt/data/BBC News Test.csv\"\n",
        "        self.language = \"en\"\n",
        "        self.max_articles = 1000\n",
        "class NewsBot2System:\n",
        "    \"\"\"Main system orchestrator for NewsBot 2.0\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.train_data = None\n",
        "        self.test_data = None\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load training and testing data\"\"\"\n",
        "        self.train_data = pd.read_csv(self.config.train_file_path)\n",
        "        self.test_data = pd.read_csv(self.config.test_file_path)\n",
        "        print(\"‚úÖ Data loaded successfully.\")\n",
        "        print(\"Train sample:\\n\", self.train_data.head())\n",
        "        print(\"Test sample:\\n\", self.test_data.head())\n",
        "\n",
        "    def analyze_article(self, article_text):\n",
        "        \"\"\"Analyze a single article ‚Äî to be expanded\"\"\"\n",
        "        return {\"status\": \"placeholder\", \"text\": article_text[:100] + \"...\"}\n",
        "\n",
        "    def process_query(self, user_query):\n",
        "        \"\"\"Process natural language queries ‚Äî placeholder\"\"\"\n",
        "        return {\"response\": f\"Query received: '{user_query}'\"}\n",
        "\n",
        "    def generate_insights(self, articles):\n",
        "        \"\"\"Generate high-level insights ‚Äî placeholder\"\"\"\n",
        "        return {\"insights\": f\"Analyzed {len(articles)} articles.\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNOLnE8US1_Y"
      },
      "source": [
        "## üìä Section 2: Advanced Content Analysis EngineThis is where you'll implement the core NLP analysis capabilities that make your NewsBot intelligent.### üéØ Section Objectives- Build enhanced text classification with confidence scoring- Implement topic modeling for content discovery- Create sentiment analysis with temporal tracking- Develop entity relationship mapping### üîó Course Module Connections- **Module 7**: Enhanced multi-class classification- **Module 8**: Advanced named entity recognition- **Module 9**: Topic modeling and clustering- **Module 6**: Sentiment analysis evolution### ü§î Key Questions to Consider1. How will you handle multiple categories per article?\n",
        "\n",
        "To support multi-label classification, NewsBot 2.0 will:\n",
        "\n",
        "Use classifiers that support multi-label outputs (e.g., One-vs-Rest Logistic Regression, BERT with sigmoid output).\n",
        "\n",
        "Apply threshold-based confidence scoring to assign one or more categories per article based on relevance.\n",
        "\n",
        "Use TF-IDF or transformer-based embeddings to understand article content holistically, capturing themes that span across categories.\n",
        "\n",
        "Allow users to view secondary or related categories, not just the primary one, to increase discoverability.\n",
        "\n",
        "This ensures that articles covering multiple subjects (e.g., politics and economics) are properly classified and surfaced under all relevant sections.\n",
        "\n",
        "2. What topics are most important to discover automatically?\n",
        "\n",
        "Important topics to discover automatically include:\n",
        "\n",
        "Emerging news events (e.g., protests, policy changes, corporate mergers)\n",
        "\n",
        "Trending themes (e.g., climate change, AI ethics, economic crises)\n",
        "\n",
        "Controversial subjects (e.g., misinformation, geopolitical conflict)\n",
        "\n",
        "Public health alerts (e.g., disease outbreaks, health policy)\n",
        "\n",
        "Industry-specific insights (e.g., tech innovation, market regulation)\n",
        "\n",
        "By using LDA, NMF, or transformer-based topic models, NewsBot can uncover these topics dynamically‚Äîeven before they dominate headlines.\n",
        "\n",
        "3. How can you track sentiment changes over time?\n",
        "\n",
        "NewsBot 2.0 will track sentiment trends by:\n",
        "\n",
        "Applying time-aware sentiment analysis on timestamped articles (e.g., daily, weekly, monthly intervals).\n",
        "\n",
        "Visualizing trends using line charts or heatmaps, separated by topic, entity, or region.\n",
        "\n",
        "Detecting anomalies or spikes in sentiment (e.g., sudden anger during a crisis).\n",
        "\n",
        "Enabling comparison across time periods (e.g., pre- vs. post-election sentiment).\n",
        "\n",
        "This approach helps identify emotional shifts, public opinion waves, and event-driven reactions in media coverage.\n",
        "\n",
        "4. What entity relationships are most valuable to extract?\n",
        "\n",
        "The most valuable entity relationships include:\n",
        "\n",
        "Person ‚Üí Organization (e.g., \"CEO of Apple\")\n",
        "\n",
        "Organization ‚Üí Event (e.g., \"sponsor of COP28\")\n",
        "\n",
        "Location ‚Üí Event (e.g., \"hosted in Paris\")\n",
        "\n",
        "Person ‚Üí Person (e.g., \"appointed by\", \"criticized by\")\n",
        "\n",
        "Entity ‚Üí Action (e.g., \"Google acquired Fitbit\")\n",
        "\n",
        "These relationships enable the construction of knowledge graphs, which are critical for answering complex queries, mapping influence networks, and generating insights from text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjCAEA2RS1_Y",
        "outputId": "094f1695-f4fc-4e7a-e174-900acd3b0765",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Advanced classification system ready for implementation!\n"
          ]
        }
      ],
      "source": [
        "# üìä Advanced Classification System\n",
        "# üìä Advanced Classification System\n",
        "\n",
        "\n",
        "class AdvancedNewsClassifier:\n",
        "    \"\"\"\n",
        "    Enhanced news classification system with:\n",
        "    - Confidence scoring\n",
        "    - Multi-label support\n",
        "    - Explainability\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes classification components.\n",
        "        You can expand this with more models or embeddings (e.g., BERT).\n",
        "        \"\"\"\n",
        "        self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "        self.model = CalibratedClassifierCV(\n",
        "            base_estimator=LogisticRegression(max_iter=1000, solver='liblinear'), cv=5\n",
        "        )\n",
        "        self.pipeline = Pipeline([\n",
        "            ('vectorizer', self.vectorizer),\n",
        "            ('classifier', self.model)\n",
        "        ])\n",
        "        self.is_trained = False\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Trains the classifier on provided text and labels.\n",
        "        Args:\n",
        "            X_train (list of str): News article texts.\n",
        "            y_train (list of str): Corresponding category labels.\n",
        "        \"\"\"\n",
        "        self.pipeline.fit(X_train, y_train)\n",
        "        self.is_trained = True\n",
        "        print(\"‚úÖ Classifier trained successfully.\")\n",
        "\n",
        "    def predict_with_confidence(self, article_text):\n",
        "        \"\"\"\n",
        "        Predicts the category of a news article with confidence scores.\n",
        "\n",
        "        Returns:\n",
        "            dict: {\n",
        "                'primary_category': str,\n",
        "                'confidence': float,\n",
        "                'alternatives': List[Tuple[str, float]]\n",
        "            }\n",
        "        \"\"\"\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"Classifier must be trained before prediction.\")\n",
        "\n",
        "        probs = self.pipeline.predict_proba([article_text])[0]\n",
        "        labels = self.pipeline.classes_\n",
        "\n",
        "        ranked = sorted(zip(labels, probs), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return {\n",
        "            'primary_category': ranked[0][0],\n",
        "            'confidence': round(ranked[0][1], 4),\n",
        "            'alternatives': [(label, round(score, 4)) for label, score in ranked[1:]]\n",
        "        }\n",
        "\n",
        "    def explain_prediction(self, article_text):\n",
        "        \"\"\"\n",
        "        Provides a basic explanation of what influenced the prediction.\n",
        "        \"\"\"\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"Classifier must be trained before explanation.\")\n",
        "\n",
        "        vector = self.vectorizer.transform([article_text])\n",
        "        feature_names = np.array(self.vectorizer.get_feature_names_out())\n",
        "        weights = self.model.base_estimator.coef_\n",
        "\n",
        "        predicted_class = self.pipeline.predict([article_text])[0]\n",
        "        class_index = list(self.model.classes_).index(predicted_class)\n",
        "\n",
        "        top_indices = np.argsort(weights[class_index])[::-1][:10]\n",
        "        top_features = feature_names[top_indices]\n",
        "\n",
        "        return {\n",
        "            'predicted_category': predicted_class,\n",
        "            'influential_features': list(top_features)\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"üìä Advanced classification system ready for implementation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV49n6fbS1_Y"
      },
      "outputs": [],
      "source": [
        "# üîç Topic Modeling and Discovery# TODO: Implement topic modeling for content discoveryclass TopicDiscoveryEngine:    \"\"\"    Advanced topic modeling for discovering themes and trends    TODO: Implement sophisticated topic analysis    \"\"\"        def __init__(self, n_topics=10, method='lda'):        # TODO: Initialize topic modeling components        # Hint: Consider:        # - LDA vs NMF vs other methods        # - Dynamic topic modeling for trend analysis        # - Hierarchical topic structures        # - Topic coherence evaluation        pass        def fit_topics(self, documents):        \"\"\"        TODO: Discover topics in document collection                Questions to consider:        - How will you preprocess text for topic modeling?        - What's the optimal number of topics?        - How will you handle topic evolution over time?        - How will you evaluate topic quality?        \"\"\"        pass        def get_article_topics(self, article_text):        \"\"\"        TODO: Get topic distribution for a single article        \"\"\"        pass        def track_topic_trends(self, articles_with_dates):        \"\"\"        TODO: Analyze how topics change over time                This is a key differentiator for your NewsBot 2.0!        Consider:        - Topic emergence and decline        - Seasonal patterns        - Event-driven topic spikes        - Cross-topic relationships        \"\"\"        pass        def visualize_topics(self):        \"\"\"        TODO: Create interactive topic visualizations                Hint: Consider using:        - pyLDAvis for LDA visualization        - Network graphs for topic relationships        - Timeline plots for topic evolution        - Word clouds for topic representation        \"\"\"        pass# TODO: Test your topic modeling# topic_engine = TopicDiscoveryEngine()print(\"üîç Topic discovery engine ready for implementation!\") # üîç Topic Modeling and Discovery\n",
        "\n",
        "class TopicDiscoveryEngine:\n",
        "    \"\"\"\n",
        "    Advanced topic modeling for discovering themes and trends\n",
        "    Uses LDA (via Gensim) with optional support for NMF in future versions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_topics=10, method='lda'):\n",
        "        self.n_topics = n_topics\n",
        "        self.method = method.lower()\n",
        "        self.dictionary = None\n",
        "        self.corpus = None\n",
        "        self.model = None\n",
        "        self.topic_keywords = []\n",
        "        self.doc_topic_distributions = []\n",
        "\n",
        "        print(f\"üß† TopicDiscoveryEngine initialized with method='{self.method}', topics={self.n_topics}\")\n",
        "\n",
        "    def preprocess_documents(self, documents):\n",
        "        \"\"\"\n",
        "        Tokenize, clean, and lemmatize documents for topic modeling.\n",
        "        \"\"\"\n",
        "        processed_docs = []\n",
        "        for doc in documents:\n",
        "            tokens = word_tokenize(doc.lower())\n",
        "            tokens = [word for word in tokens if word.isalpha()]\n",
        "            tokens = [word for word in tokens if word not in self.config.stop_words]\n",
        "            lemmatized = [self.config.lemmatizer.lemmatize(word) for word in tokens]\n",
        "            processed_docs.append(lemmatized)\n",
        "        return processed_docs\n",
        "\n",
        "    def fit_topics(self, documents):\n",
        "        \"\"\"\n",
        "        Discover topics in a collection of documents.\n",
        "        Builds LDA model and stores corpus, dictionary, and topic keywords.\n",
        "        \"\"\"\n",
        "        print(\"üîé Fitting topic model...\")\n",
        "\n",
        "        processed_docs = self.preprocess_documents(documents)\n",
        "        self.dictionary = corpora.Dictionary(processed_docs)\n",
        "        self.corpus = [self.dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "        if self.method == 'lda':\n",
        "            self.model = gensim.models.LdaModel(\n",
        "                self.corpus,\n",
        "                num_topics=self.n_topics,\n",
        "                id2word=self.dictionary,\n",
        "                passes=10,\n",
        "                random_state=42\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(\"Only LDA is currently supported.\")\n",
        "\n",
        "        # Store top words for each topic\n",
        "        self.topic_keywords = [\n",
        "            [word for word, _ in self.model.show_topic(topic_id, topn=10)]\n",
        "            for topic_id in range(self.n_topics)\n",
        "        ]\n",
        "\n",
        "        print(\"‚úÖ Topics discovered:\")\n",
        "        for i, topic in enumerate(self.topic_keywords):\n",
        "            print(f\"  Topic {i+1}: {', '.join(topic)}\")\n",
        "\n",
        "    def get_article_topics(self, article_text):\n",
        "        \"\"\"\n",
        "        Get the topic distribution for a single article.\n",
        "        Returns list of (topic_id, score) pairs.\n",
        "        \"\"\"\n",
        "        tokens = self.preprocess_documents([article_text])[0]\n",
        "        bow = self.dictionary.doc2bow(tokens)\n",
        "        topic_dist = self.model.get_document_topics(bow)\n",
        "        return sorted(topic_dist, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    def track_topic_trends(self, articles_with_dates):\n",
        "        \"\"\"\n",
        "        Analyze topic frequencies over time.\n",
        "        Expects a list of tuples: (article_text, date_string)\n",
        "        Returns dictionary: {topic_id: [counts_by_time]}\n",
        "        \"\"\"\n",
        "        print(\"üìà Tracking topic trends (function scaffold only).\")\n",
        "        # Placeholder logic\n",
        "        return {}\n",
        "\n",
        "    def visualize_topics(self):\n",
        "        \"\"\"\n",
        "        Basic wordcloud visualization of each topic.\n",
        "        Replace/extend with pyLDAvis, network plots, or timelines.\n",
        "        \"\"\"\n",
        "        print(\"üñºÔ∏è Generating word clouds for topics...\")\n",
        "\n",
        "        for i, words in enumerate(self.topic_keywords):\n",
        "            word_freq = {word: 1 for word in words}  # dummy frequency\n",
        "            wc = WordCloud(width=600, height=300, background_color='white').generate_from_frequencies(word_freq)\n",
        "\n",
        "            plt.figure(figsize=(8, 4))\n",
        "            plt.imshow(wc, interpolation='bilinear')\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"Topic {i+1}\")\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WdyTahqS1_Y"
      },
      "outputs": [],
      "source": [
        "# üé≠ Advanced Sentiment Analysis# TODO: Implement sentiment analysis with temporal trackingclass SentimentEvolutionTracker:    \"\"\"    Advanced sentiment analysis with temporal and contextual understanding    TODO: Build sophisticated sentiment tracking    \"\"\"        def __init__(self):        # TODO: Initialize sentiment analysis components        # Hint: Consider:        # - Multiple sentiment dimensions (emotion, subjectivity, etc.)        # - Domain-specific sentiment models        # - Aspect-based sentiment analysis        # - Temporal sentiment patterns        pass        def analyze_sentiment(self, article_text):        \"\"\"        TODO: Comprehensive sentiment analysis                Should return:        - Overall sentiment (positive/negative/neutral)        - Confidence score        - Emotional dimensions (joy, anger, fear, etc.)        - Aspect-based sentiments (if applicable)        - Key phrases driving sentiment        \"\"\"        pass        def track_sentiment_over_time(self, articles_with_dates):        \"\"\"        TODO: Analyze sentiment trends over time                This is crucial for understanding public opinion evolution!        Consider:        - Daily/weekly/monthly sentiment trends        - Event-driven sentiment changes        - Topic-specific sentiment evolution        - Comparative sentiment across sources        \"\"\"        pass        def detect_sentiment_anomalies(self, sentiment_timeline):        \"\"\"        TODO: Identify unusual sentiment patterns                This could help detect:        - Breaking news events        - Public opinion shifts        - Misinformation campaigns        - Crisis situations        \"\"\"        pass# TODO: Test your sentiment tracker# sentiment_tracker = SentimentEvolutionTracker()print(\"üé≠ Sentiment evolution tracker ready for implementation!\")# üé≠ Advanced Sentiment Analysis\n",
        "\n",
        "class SentimentEvolutionTracker:\n",
        "    \"\"\"\n",
        "    Advanced sentiment analysis with temporal and contextual understanding.\n",
        "    Supports polarity, subjectivity, emotions, and time-based trend analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"üìä SentimentEvolutionTracker initialized!\")\n",
        "        self.timeline_data = []\n",
        "\n",
        "    def analyze_sentiment(self, article_text):\n",
        "        \"\"\"\n",
        "        Perform comprehensive sentiment analysis on one article.\n",
        "\n",
        "        Returns:\n",
        "        - Overall sentiment label\n",
        "        - Confidence score (abs(polarity))\n",
        "        - Subjectivity\n",
        "        - Emotional dimensions (via NRCLex)\n",
        "        - Key phrases (noun chunks)\n",
        "        \"\"\"\n",
        "        blob = TextBlob(article_text)\n",
        "        polarity = blob.sentiment.polarity\n",
        "        subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "        # Sentiment label\n",
        "        if polarity > 0.1:\n",
        "            sentiment_label = \"positive\"\n",
        "        elif polarity < -0.1:\n",
        "            sentiment_label = \"negative\"\n",
        "        else:\n",
        "            sentiment_label = \"neutral\"\n",
        "\n",
        "        # Emotion detection\n",
        "        emotion_obj = NRCLex(article_text)\n",
        "        emotions = dict(Counter(emotion_obj.raw_emotion_scores))\n",
        "\n",
        "        # Key phrases\n",
        "        key_phrases = list(set(blob.noun_phrases))\n",
        "\n",
        "        return {\n",
        "            \"sentiment\": sentiment_label,\n",
        "            \"confidence\": abs(polarity),\n",
        "            \"subjectivity\": subjectivity,\n",
        "            \"emotions\": emotions,\n",
        "            \"key_phrases\": key_phrases\n",
        "        }\n",
        "\n",
        "    def track_sentiment_over_time(self, articles_with_dates):\n",
        "        \"\"\"\n",
        "        Analyze sentiment trends over time.\n",
        "\n",
        "        Input: list of (article_text, date_string)\n",
        "        Output: DataFrame with date, polarity, subjectivity, emotion scores\n",
        "        \"\"\"\n",
        "        records = []\n",
        "\n",
        "        for text, date in articles_with_dates:\n",
        "            blob = TextBlob(text)\n",
        "            polarity = blob.sentiment.polarity\n",
        "            subjectivity = blob.sentiment.subjectivity\n",
        "            emotion_obj = NRCLex(text)\n",
        "            emotions = dict(Counter(emotion_obj.raw_emotion_scores))\n",
        "\n",
        "            record = {\n",
        "                \"date\": pd.to_datetime(date),\n",
        "                \"polarity\": polarity,\n",
        "                \"subjectivity\": subjectivity,\n",
        "                **emotions\n",
        "            }\n",
        "\n",
        "            records.append(record)\n",
        "\n",
        "        df = pd.DataFrame(records)\n",
        "        df = df.sort_values(\"date\")\n",
        "        self.timeline_data = df\n",
        "        return df\n",
        "\n",
        "    def detect_sentiment_anomalies(self, sentiment_timeline, window=5, threshold=2.0):\n",
        "        \"\"\"\n",
        "        Detect major sentiment spikes using rolling z-score.\n",
        "\n",
        "        Input:\n",
        "        - sentiment_timeline: DataFrame from track_sentiment_over_time\n",
        "        - window: smoothing window\n",
        "        - threshold: z-score threshold for anomaly\n",
        "\n",
        "        Output:\n",
        "        - DataFrame with anomaly flag\n",
        "        \"\"\"\n",
        "        df = sentiment_timeline.copy()\n",
        "        df['rolling_mean'] = df['polarity'].rolling(window).mean()\n",
        "        df['rolling_std'] = df['polarity'].rolling(window).std()\n",
        "        df['z_score'] = (df['polarity'] - df['rolling_mean']) / df['rolling_std']\n",
        "        df['anomaly'] = df['z_score'].abs() > threshold\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rms0L15GS1_Y",
        "outputId": "e2dad21d-ba0e-4b24-a55f-4d942f60132b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üï∏Ô∏è Entity relationship mapper ready for implementation!\n"
          ]
        }
      ],
      "source": [
        "# üï∏Ô∏è Entity Relationship Mapping# TODO: Implement advanced entity recognition and relationship mappingclass EntityRelationshipMapper:    \"\"\"    Advanced NER with relationship extraction and network analysis    TODO: Build sophisticated entity understanding    \"\"\"        def __init__(self):        # TODO: Initialize NER and relationship extraction components        # Hint: Consider:        # - Multiple NER models (spaCy, transformers, custom)        # - Relationship extraction techniques        # - Entity linking and disambiguation        # - Knowledge graph construction        pass        def extract_entities(self, article_text):        \"\"\"        TODO: Extract and classify entities                Should identify:        - People (with roles/titles)        - Organizations (with types)        - Locations (with hierarchies)        - Events (with dates/contexts)        - Products, technologies, etc.        \"\"\"        pass        def extract_relationships(self, article_text):        \"\"\"        TODO: Extract relationships between entities                Examples:        - \"CEO of\" (person -> organization)        - \"located in\" (organization -> location)        - \"acquired by\" (organization -> organization)        - \"attended\" (person -> event)        \"\"\"        pass        def build_knowledge_graph(self, articles):        \"\"\"        TODO: Build knowledge graph from multiple articles                This creates a network of entities and relationships        that can reveal:        - Key players in different domains        - Hidden connections between entities        - Influence networks        - Trending relationships        \"\"\"        pass        def find_entity_connections(self, entity1, entity2):        \"\"\"        TODO: Find connections between two entities                This could help answer questions like:        - \"How are Apple and Tesla connected?\"        - \"What's the relationship between Biden and climate change?\"        \"\"\"        pass# TODO: Test your entity mapper# entity_mapper = EntityRelationshipMapper()print(\"üï∏Ô∏è Entity relationship mapper ready for implementation!\")# üï∏Ô∏è Entity Relationship Mapping\n",
        "\n",
        "# üï∏Ô∏è Entity Relationship Mapping\n",
        "\n",
        "class EntityRelationshipMapper:\n",
        "    \"\"\"\n",
        "    Advanced NER with relationship extraction and network analysis.\n",
        "    Extracts named entities, builds a relationship graph, and finds entity-level connections.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.graph = nx.Graph()\n",
        "\n",
        "    def extract_entities(self, article_text):\n",
        "        \"\"\"\n",
        "        Extract and classify entities using spaCy.\n",
        "        Identifies:\n",
        "        - People (with roles/titles)\n",
        "        - Organizations (with types)\n",
        "        - Locations (with hierarchies)\n",
        "        - Events (with dates/contexts)\n",
        "        - Products, technologies, etc.\n",
        "        \"\"\"\n",
        "        doc = self.nlp(article_text)\n",
        "        entities = defaultdict(set)\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            entities[ent.label_].add(ent.text)\n",
        "\n",
        "        return {label: list(values) for label, values in entities.items()}\n",
        "\n",
        "    def extract_relationships(self, article_text):\n",
        "        \"\"\"\n",
        "        Extract relationships between entities using rule-based heuristics.\n",
        "        Examples include:\n",
        "        - \"CEO of\" (person -> organization)\n",
        "        - \"located in\" (organization -> location)\n",
        "        - \"acquired by\" (organization -> organization)\n",
        "        - \"attended\" (person -> event)\n",
        "        \"\"\"\n",
        "        doc = self.nlp(article_text)\n",
        "        relationships = []\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            ents = list(sent.ents)\n",
        "            for i, ent1 in enumerate(ents):\n",
        "                for j, ent2 in enumerate(ents):\n",
        "                    if i != j:\n",
        "                        if \"CEO\" in sent.text and ent1.label_ == \"PERSON\" and ent2.label_ == \"ORG\":\n",
        "                            relationships.append((ent1.text, \"CEO of\", ent2.text))\n",
        "                        elif \"acquired\" in sent.text and ent1.label_ == \"ORG\" and ent2.label_ == \"ORG\":\n",
        "                            relationships.append((ent1.text, \"acquired\", ent2.text))\n",
        "                        elif \"located in\" in sent.text and ent1.label_ == \"ORG\" and ent2.label_ == \"GPE\":\n",
        "                            relationships.append((ent1.text, \"located in\", ent2.text))\n",
        "                        elif \"attended\" in sent.text and ent1.label_ == \"PERSON\" and ent2.label_ == \"EVENT\":\n",
        "                            relationships.append((ent1.text, \"attended\", ent2.text))\n",
        "\n",
        "        return relationships\n",
        "\n",
        "    def build_knowledge_graph(self, articles):\n",
        "        \"\"\"\n",
        "        Build a knowledge graph from multiple articles.\n",
        "        Constructs a network of entities and relationships for analysis.\n",
        "        \"\"\"\n",
        "        for article in articles:\n",
        "            relationships = self.extract_relationships(article)\n",
        "            for ent1, relation, ent2 in relationships:\n",
        "                self.graph.add_node(ent1)\n",
        "                self.graph.add_node(ent2)\n",
        "                self.graph.add_edge(ent1, ent2, label=relation)\n",
        "\n",
        "    def find_entity_connections(self, entity1, entity2):\n",
        "        \"\"\"\n",
        "        Find and return the shortest relationship path between two entities.\n",
        "        Useful for questions like:\n",
        "        - \"How are Apple and Tesla connected?\"\n",
        "        - \"What's the relationship between Biden and climate change?\"\n",
        "        \"\"\"\n",
        "        if nx.has_path(self.graph, entity1, entity2):\n",
        "            path = nx.shortest_path(self.graph, entity1, entity2)\n",
        "            connections = []\n",
        "            for i in range(len(path) - 1):\n",
        "                edge_data = self.graph.get_edge_data(path[i], path[i + 1])\n",
        "                label = edge_data.get(\"label\", \"related to\")\n",
        "                connections.append((path[i], label, path[i + 1]))\n",
        "            return connections\n",
        "        return []\n",
        "\n",
        "print(\"üï∏Ô∏è Entity relationship mapper ready for implementation!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIzXABp5S1_Y"
      },
      "source": [
        "## üß† Section 3: Language Understanding & GenerationThis section focuses on advanced language model integration for summarization, content enhancement, and semantic understanding.### üéØ Section Objectives- Implement intelligent text summarization- Build content enhancement and expansion capabilities- Create semantic search and similarity matching- Develop query understanding and expansion### üîó Course Module Connections- **Module 10**: Neural networks and language models- **Module 11**: Advanced text generation techniques- **Module 12**: Natural language understanding### ü§î Key Questions to Consider1. 1. What makes a good summary for different types of news?\n",
        "\n",
        "A good summary depends on the news type:\n",
        "\n",
        "Breaking news: Should be brief, fact-focused, and convey the who, what, where, and when clearly.\n",
        "\n",
        "Analytical articles: Should highlight key arguments, perspectives, and evidence supporting the analysis.\n",
        "\n",
        "Feature stories: Should capture the tone, narrative arc, and human elements while still preserving key facts.\n",
        "\n",
        "Opinion pieces: Should reflect the central thesis and main supporting points, not necessarily neutral facts.\n",
        "\n",
        "Across all types, a good summary should be:\n",
        "\n",
        "Accurate (factually correct)\n",
        "\n",
        "Concise (no unnecessary detail)\n",
        "\n",
        "Coherent (logically structured)\n",
        "\n",
        "Context-aware (preserving relevance and tone)\n",
        "\n",
        "2. How can you enhance articles with relevant context?\n",
        "\n",
        "Articles can be enhanced by:\n",
        "\n",
        "Linking to background information on key people, events, or terminology\n",
        "\n",
        "Adding timelines of past related developments\n",
        "\n",
        "Inserting expert quotes or commentary from trusted sources\n",
        "\n",
        "Highlighting statistics or reports to support claims\n",
        "\n",
        "Fact-checking and flagging misleading or disputed content\n",
        "\n",
        "Cross-referencing with articles in other languages or regions to present diverse perspectives\n",
        "\n",
        "This turns static articles into richer, more informative, and trustworthy content.\n",
        "\n",
        "3. What semantic relationships are most valuable to capture?\n",
        "\n",
        "The most valuable semantic relationships include:\n",
        "\n",
        "Cause-effect (e.g., ‚ÄúX led to Y‚Äù)\n",
        "\n",
        "Entity relationships (e.g., ‚ÄúPerson A is CEO of Company B‚Äù)\n",
        "\n",
        "Comparisons (e.g., ‚ÄúX is more effective than Y‚Äù)\n",
        "\n",
        "Temporal events (e.g., sequence of developments)\n",
        "\n",
        "Contrasts and contradictions (to highlight differing viewpoints)\n",
        "\n",
        "Sentiment and stance (e.g., detecting opinion, bias, or support)\n",
        "\n",
        "These relationships help NewsBot go beyond surface-level content to infer insights and connect information meaningfully.\n",
        "\n",
        "4. How will you handle ambiguous or complex queries?\n",
        "\n",
        "To handle complex queries, NewsBot 2.0 will:\n",
        "\n",
        "Use semantic parsing to understand intent and extract key entities\n",
        "\n",
        "Apply query expansion (e.g., synonyms, related concepts) to broaden search\n",
        "\n",
        "Disambiguate based on context (e.g., recent news, previous queries, user preferences)\n",
        "\n",
        "Handle multi-part questions by breaking them down\n",
        "\n",
        "Leverage language models (e.g., transformers) for natural language understanding\n",
        "\n",
        "Return explanations or ask for clarification when queries are too vague\n",
        "\n",
        "The goal is to make NewsBot feel conversational, precise, and informative, even under uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mFCsBZwS1_Z",
        "outputId": "78b97a04-b415-4495-b0e2-70c066c8fc13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Intelligent summarizer ready for implementation!\n"
          ]
        }
      ],
      "source": [
        "# üìù Intelligent Text Summarization# TODO: Implement advanced summarization capabilitiesclass IntelligentSummarizer:    \"\"\"    Advanced text summarization with multiple strategies and quality control    TODO: Build sophisticated summarization system    \"\"\"        def __init__(self):        # TODO: Initialize summarization models        # Hint: Consider:        # - Extractive vs abstractive summarization        # - Pre-trained models (BART, T5, etc.)        # - Domain-specific fine-tuning        # - Multi-document summarization        # - Quality assessment metrics        pass        def summarize_article(self, article_text, summary_type='balanced'):        \"\"\"        TODO: Generate high-quality article summary                Parameters:        - summary_type: 'brief', 'balanced', 'detailed'                Should consider:        - Article length and complexity        - Key information preservation        - Readability and coherence        - Factual accuracy        \"\"\"        pass        def summarize_multiple_articles(self, articles, focus_topic=None):        \"\"\"        TODO: Create unified summary from multiple articles                This is particularly valuable for:        - Breaking news coverage        - Topic-based summaries        - Trend analysis        - Comparative reporting        \"\"\"        pass        def generate_headlines(self, article_text):        \"\"\"        TODO: Generate compelling headlines                Consider different styles:        - Informative headlines        - Engaging headlines        - SEO-optimized headlines        - Social media headlines        \"\"\"        pass        def assess_summary_quality(self, original_text, summary):        \"\"\"        TODO: Evaluate summary quality                Metrics to consider:        - ROUGE scores        - Factual consistency        - Readability scores        - Information coverage        \"\"\"        pass# TODO: Test your summarizer# summarizer = IntelligentSummarizer()print(\"üìù Intelligent summarizer ready for implementation!\")\n",
        "# üìù Intelligent Text Summarization\n",
        "\n",
        "class IntelligentSummarizer:\n",
        "    \"\"\"\n",
        "    Advanced text summarization with multiple strategies and quality control.\n",
        "    Supports:\n",
        "    - Extractive and abstractive summarization\n",
        "    - Multi-document summarization\n",
        "    - Headline generation\n",
        "    - Summary quality assessment\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.model_name = \"facebook/bart-large-cnn\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)\n",
        "        self.summarizer = pipeline(\"summarization\", model=self.model, tokenizer=self.tokenizer)\n",
        "\n",
        "    def summarize_article(self, article_text, summary_type='balanced'):\n",
        "        \"\"\"\n",
        "        Generate a high-quality article summary.\n",
        "        Parameters:\n",
        "            - summary_type: 'brief', 'balanced', or 'detailed'\n",
        "        \"\"\"\n",
        "        length_map = {\n",
        "            'brief': (30, 60),\n",
        "            'balanced': (80, 130),\n",
        "            'detailed': (150, 250)\n",
        "        }\n",
        "        min_len, max_len = length_map.get(summary_type, (80, 130))\n",
        "\n",
        "        summary = self.summarizer(\n",
        "            article_text,\n",
        "            min_length=min_len,\n",
        "            max_length=max_len,\n",
        "            do_sample=False,\n",
        "            truncation=True\n",
        "        )[0]['summary_text']\n",
        "\n",
        "        return summary.strip()\n",
        "\n",
        "    def summarize_multiple_articles(self, articles, focus_topic=None):\n",
        "        \"\"\"\n",
        "        Create a unified summary from multiple articles.\n",
        "        Optionally filter sentences related to a focus_topic.\n",
        "        \"\"\"\n",
        "        combined = \" \".join(articles)\n",
        "        if focus_topic:\n",
        "            filtered = [s for s in TextBlob(combined).sentences if focus_topic.lower() in s.lower()]\n",
        "            if filtered:\n",
        "                combined = \" \".join([str(s) for s in filtered])\n",
        "        return self.summarize_article(combined, summary_type='balanced')\n",
        "\n",
        "    def generate_headlines(self, article_text):\n",
        "        \"\"\"\n",
        "        Generate multiple headline styles.\n",
        "        \"\"\"\n",
        "        summary = self.summarize_article(article_text, summary_type='brief')\n",
        "        return {\n",
        "            \"informative\": summary,\n",
        "            \"engaging\": f\"Breaking: {summary[:60]}...\",\n",
        "            \"seo\": f\"{summary.split()[0]}: {summary[:80]}\",\n",
        "            \"social\": f\"üî• {summary[:100]} #news\"\n",
        "        }\n",
        "\n",
        "    def assess_summary_quality(self, original_text, summary):\n",
        "        \"\"\"\n",
        "        Evaluate summary quality using:\n",
        "        - ROUGE scores\n",
        "        - Readability scores\n",
        "        - Reading grade level\n",
        "        \"\"\"\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        rouge_scores = scorer.score(original_text, summary)\n",
        "\n",
        "        readability = textstat.flesch_reading_ease(summary)\n",
        "        reading_grade = textstat.text_standard(summary, float_output=True)\n",
        "\n",
        "        return {\n",
        "            \"rouge\": rouge_scores,\n",
        "            \"readability_score\": readability,\n",
        "            \"reading_grade_level\": reading_grade\n",
        "        }\n",
        "\n",
        "print(\"üìù Intelligent summarizer ready for implementation!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H20vkk9mS1_Z",
        "outputId": "06d1bca8-5c24-4384-b5e1-ce6c275b68e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Semantic search engine fully implemented and ready to use.\n"
          ]
        }
      ],
      "source": [
        "# üîç Semantic Search and Similarity# TODO: Implement semantic understanding and search capabilitiesclass SemanticSearchEngine:    \"\"\"    Advanced semantic search using embeddings and similarity matching    TODO: Build sophisticated semantic understanding    \"\"\"        def __init__(self):        # TODO: Initialize semantic search components        # Hint: Consider:        # - Pre-trained embeddings (Word2Vec, GloVe, BERT)        # - Sentence-level embeddings        # - Document-level embeddings        # - Vector databases for efficient search        # - Similarity metrics and thresholds        pass        def encode_documents(self, documents):        \"\"\"        TODO: Convert documents to semantic embeddings                This creates vector representations that capture meaning        beyond just keyword matching        \"\"\"        pass        def find_similar_articles(self, query_article, top_k=5):        \"\"\"        TODO: Find semantically similar articles                This should find articles that are:        - Topically related        - Contextually similar        - Complementary in information        \"\"\"        pass        def semantic_search(self, query_text, article_database):        \"\"\"        TODO: Search articles using natural language queries                Examples:        - \"Articles about climate change policy\"        - \"Technology companies facing regulation\"        - \"Economic impact of pandemic\"        \"\"\"        pass        def cluster_similar_content(self, articles):        \"\"\"        TODO: Group articles by semantic similarity                This can help:        - Organize large article collections        - Identify story clusters        - Detect duplicate or near-duplicate content        - Find complementary perspectives        \"\"\"        pass# TODO: Test your semantic search# search_engine = SemanticSearchEngine()print(\"üîç Semantic search engine ready for implementation!\")\n",
        "# üîç Semantic Search and Similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class SemanticSearchEngine:\n",
        "    \"\"\"\n",
        "    Advanced semantic search using embeddings and similarity matching.\n",
        "    Supports:\n",
        "    - Semantic document encoding\n",
        "    - Query-based semantic retrieval\n",
        "    - Document similarity lookup\n",
        "    - Clustering and grouping by meaning\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        self.embeddings = None\n",
        "        self.documents = []\n",
        "        print(\"üîç Sentence embedding model loaded.\")\n",
        "\n",
        "    def encode_documents(self, documents):\n",
        "        \"\"\"\n",
        "        Convert documents to semantic embeddings.\n",
        "        Returns:\n",
        "            - Matrix of embeddings\n",
        "        \"\"\"\n",
        "        self.documents = documents\n",
        "        self.embeddings = self.model.encode(documents, convert_to_tensor=True)\n",
        "        return self.embeddings\n",
        "\n",
        "    def find_similar_articles(self, query_article, top_k=5):\n",
        "        \"\"\"\n",
        "        Find articles most similar to a given article.\n",
        "        Returns:\n",
        "            - List of (document_text, similarity_score)\n",
        "        \"\"\"\n",
        "        if self.embeddings is None:\n",
        "            raise ValueError(\"Documents must be encoded first using encode_documents().\")\n",
        "\n",
        "        query_embedding = self.model.encode([query_article], convert_to_tensor=True)\n",
        "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
        "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "        return [(self.documents[i], float(similarities[i])) for i in top_indices]\n",
        "\n",
        "    def semantic_search(self, query_text, article_database, top_k=5):\n",
        "        \"\"\"\n",
        "        Perform semantic search over article database.\n",
        "        Returns:\n",
        "            - List of (article_text, similarity_score)\n",
        "        \"\"\"\n",
        "        self.encode_documents(article_database)\n",
        "        query_embedding = self.model.encode([query_text], convert_to_tensor=True)\n",
        "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
        "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "        return [(self.documents[i], float(similarities[i])) for i in top_indices]\n",
        "\n",
        "    def cluster_similar_content(self, articles, num_clusters=5):\n",
        "        \"\"\"\n",
        "        Group articles into semantic clusters.\n",
        "        Returns:\n",
        "            - Dictionary of clusters {cluster_id: [articles]}\n",
        "        \"\"\"\n",
        "        vectors = self.model.encode(articles)\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "        labels = kmeans.fit_predict(vectors)\n",
        "\n",
        "        clusters = defaultdict(list)\n",
        "        for idx, label in enumerate(labels):\n",
        "            clusters[label].append(articles[idx])\n",
        "\n",
        "        return dict(clusters)\n",
        "\n",
        "print(\"üîç Semantic search engine fully implemented and ready to use.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9SzynUOS1_Z"
      },
      "outputs": [],
      "source": [
        "# üí° Content Enhancement and Insights# TODO: Implement content enhancement and automatic insight generation\n",
        "class ContentEnhancer:\n",
        "    \"\"\"\n",
        "    Advanced content analysis and enhancement system.\n",
        "    Provides intelligent content augmentation, including contextual enrichment,\n",
        "    insight generation, and fact-checking.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # TODO: Initialize content enhancement components\n",
        "        # Hint: Consider:\n",
        "        # - Knowledge bases and external APIs\n",
        "        # - Fact-checking capabilities\n",
        "        # - Context enrichment\n",
        "        # - Trend analysis\n",
        "        # - Comparative analysis\n",
        "        print(\"üí° ContentEnhancer initialized.\")\n",
        "\n",
        "    def enhance_article(self, article_text):\n",
        "        \"\"\"\n",
        "        TODO: Add valuable context and insights to articles\n",
        "\n",
        "        Enhancements might include:\n",
        "        - Background information on key entities\n",
        "        - Related historical events\n",
        "        - Statistical context\n",
        "        - Expert opinions or analysis\n",
        "        - Fact-checking results\n",
        "        \"\"\"\n",
        "        # Placeholder for enhancement logic\n",
        "        return f\"Enhanced version of: {article_text[:100]}...\"\n",
        "\n",
        "    def generate_insights(self, articles):\n",
        "        \"\"\"\n",
        "        TODO: Generate high-level insights from article collection\n",
        "\n",
        "        Insights might include:\n",
        "        - Emerging trends and patterns\n",
        "        - Contradictory information\n",
        "        - Missing perspectives\n",
        "        - Key stakeholders and their positions\n",
        "        - Potential implications or consequences\n",
        "        \"\"\"\n",
        "        insights = [\"üìä Generated Insights:\"]\n",
        "\n",
        "        # Basic trend analysis (example: most frequent terms)\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "\n",
        "        all_text = \" \".join(articles).lower()\n",
        "        keywords = self.vectorizer.fit_transform([all_text])\n",
        "        vocab = self.vectorizer.get_feature_names_out()\n",
        "        counts = np.asarray(keywords.sum(axis=0)).flatten()\n",
        "        top_terms = sorted(zip(vocab, counts), key=lambda x: -x[1])[:5]\n",
        "\n",
        "        insights.append(\"üìà Trending Topics:\")\n",
        "        for term, count in top_terms:\n",
        "            insights.append(f\"- {term} (mentioned {count} times)\")\n",
        "\n",
        "        # Basic contradiction detector\n",
        "        if \"support\" in all_text and \"oppose\" in all_text:\n",
        "            insights.append(\"‚ö†Ô∏è Conflicting positions detected: Articles may contain both support and opposition.\")\n",
        "\n",
        "        return \"\\n\".join(insights)\n",
        "\n",
        "    def detect_information_gaps(self, articles, topic):\n",
        "        \"\"\"\n",
        "        Identify missing or underreported aspects of a topic.\n",
        "        \"\"\"\n",
        "        joined = \" \".join(articles).lower()\n",
        "        gaps = []\n",
        "\n",
        "        if topic.lower() == \"climate change\":\n",
        "            if \"carbon tax\" not in joined:\n",
        "                gaps.append(\"üí° Missing mention of carbon tax policy.\")\n",
        "            if \"IPCC\" not in joined:\n",
        "                gaps.append(\"üí° No reference to IPCC (Intergovernmental Panel on Climate Change).\")\n",
        "        if topic.lower() == \"ai\":\n",
        "            if \"ethics\" not in joined:\n",
        "                gaps.append(\"üí° No discussion of AI ethics.\")\n",
        "            if \"regulation\" not in joined:\n",
        "                gaps.append(\"üí° Lacks mention of AI regulation or oversight.\")\n",
        "\n",
        "        if not gaps:\n",
        "            return \"‚úÖ No major information gaps detected.\"\n",
        "        return \"\\n\".join(gaps)\n",
        "\n",
        "    def cross_reference_facts(self, article_text):\n",
        "        \"\"\"\n",
        "        Simulate fact-checking of article content.\n",
        "        (In real deployment, integrate external APIs)\n",
        "        \"\"\"\n",
        "        facts = {\n",
        "            \"COVID-19 started in 2019\": True,\n",
        "            \"Earth is flat\": False,\n",
        "            \"AI is conscious\": False,\n",
        "        }\n",
        "\n",
        "        results = []\n",
        "        for claim, valid in facts.items():\n",
        "            if claim.lower() in article_text.lower():\n",
        "                results.append(f\"‚úÖ Fact-check: '{claim}' is {'true' if valid else 'false'}\")\n",
        "\n",
        "        if not results:\n",
        "            return \"‚ÑπÔ∏è No verifiable claims detected.\"\n",
        "        return \"\\n\".join(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt2bH3pHS1_Z"
      },
      "source": [
        "## üåç Section 4: Multilingual IntelligenceThis section focuses on handling multiple languages and cross-cultural analysis - a key differentiator for NewsBot 2.0.### üéØ Section Objectives- Implement automatic language detection- Build translation and cross-lingual analysis capabilities- Create cultural context understanding- Develop comparative analysis across languages### üîó Course Module Connections- **Module 11**: Machine translation and multilingual processing- **Module 8**: Cross-lingual named entity recognition- **Module 9**: Multilingual topic modeling### ü§î Key Questions to Consider1. 1. What languages are most important for your use case?\n",
        "\n",
        "The most important languages depend on NewsBot 2.0's target audience and global relevance. For a general-purpose system, key languages include:\n",
        "\n",
        "English ‚Äì Dominant in international journalism and global news.\n",
        "\n",
        "Spanish ‚Äì Widely spoken in the Americas and Europe.\n",
        "\n",
        "Mandarin Chinese ‚Äì Critical for coverage of East Asia.\n",
        "\n",
        "Arabic ‚Äì Covers key geopolitical regions (Middle East, North Africa).\n",
        "\n",
        "French ‚Äì Common across Africa, Europe, and parts of Canada.\n",
        "\n",
        "Hindi ‚Äì For coverage in India, a major news market.\n",
        "\n",
        "Russian ‚Äì Important for regional and geopolitical insights.\n",
        "\n",
        "Additionally, NewsBot should support automatic expansion to other languages based on user demand and global events.\n",
        "\n",
        "2. How will you handle cultural nuances and context?\n",
        "\n",
        "NewsBot 2.0 will handle cultural nuances by:\n",
        "\n",
        "Using language-specific preprocessing to preserve idioms, expressions, and grammar.\n",
        "\n",
        "Applying cultural context databases or external knowledge bases to interpret meaning beyond translation.\n",
        "\n",
        "Identifying region-specific references (e.g., holidays, political systems, societal norms).\n",
        "\n",
        "Adjusting sentiment and summarization tools to account for cultural variation in tone and emotional expression.\n",
        "\n",
        "Incorporating cross-lingual Named Entity Recognition (NER) to recognize local figures, events, or places accurately.\n",
        "\n",
        "This ensures the system does more than just translate‚Äîit interprets and respects cultural meaning.\n",
        "\n",
        "3. What insights can you gain from cross-language comparison?\n",
        "\n",
        "Cross-lingual analysis enables NewsBot to:\n",
        "\n",
        "Compare media bias or framing between regions (e.g., how different countries report the same event)\n",
        "\n",
        "Spot emerging topics in non-English media before they trend globally\n",
        "\n",
        "Reveal underreported issues by tracking coverage differences\n",
        "\n",
        "Understand public sentiment across countries or cultures\n",
        "\n",
        "Highlight conflicting narratives between international outlets\n",
        "\n",
        "This makes NewsBot 2.0 more powerful for journalists, analysts, and global readers.\n",
        "\n",
        "4. How will you ensure translation quality and accuracy?\n",
        "\n",
        "To ensure translation quality:\n",
        "\n",
        "Use multiple translation engines (e.g., Google, DeepL) and compare outputs.\n",
        "\n",
        "Implement quality scoring based on readability, fluency, and semantic similarity.\n",
        "\n",
        "Apply post-editing techniques like grammar correction or style tuning.\n",
        "\n",
        "Train or fine-tune domain-specific models for news-related text.\n",
        "\n",
        "Validate critical translations (e.g., headlines or quotes) using human-in-the-loop review or fact-checking tools.\n",
        "\n",
        "These steps ensure translations are not just linguistically correct but contextually accurate and trustworthy.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EAZsU6mS1_Z",
        "outputId": "c0f967ab-0482-4c5f-e53e-46bac5cc9040",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Multilingual processor fully implemented and ready to use.\n"
          ]
        }
      ],
      "source": [
        "# üåê Language Detection and Processing# TODO: Implement multilingual capabilitiesclass MultilingualProcessor:    \"\"\"    Advanced multilingual processing with language detection and cultural context    TODO: Build sophisticated multilingual understanding    \"\"\"        def __init__(self):        # TODO: Initialize multilingual components        # Hint: Consider:        # - Language detection models        # - Translation services (Google, Azure, etc.)        # - Multilingual embeddings        # - Cultural context databases        # - Cross-lingual NER models        pass        def detect_language(self, text):        \"\"\"        TODO: Detect language with confidence scoring                Should handle:        - Multiple languages in same text        - Short text snippets        - Code-switching        - Confidence thresholds        \"\"\"        pass        def translate_text(self, text, target_language='en'):        \"\"\"        TODO: High-quality translation with quality assessment                Consider:        - Multiple translation services        - Quality scoring        - Context preservation        - Cultural adaptation        \"\"\"        pass        def analyze_cross_lingual(self, articles_by_language):        \"\"\"        TODO: Compare coverage and perspectives across languages                This could reveal:        - Different cultural perspectives        - Varying coverage depth        - Regional biases        - Information gaps        \"\"\"        pass        def extract_cultural_context(self, text, source_language):        \"\"\"        TODO: Identify cultural references and context                This helps understand:        - Cultural idioms and expressions        - Regional references        - Historical context        - Social and political nuances        \"\"\"        pass# TODO: Test your multilingual processor# multilingual = MultilingualProcessor()print(\"üåê Multilingual processor ready for implementation!\")\n",
        "from langdetect import detect_langs\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "class MultilingualProcessor:\n",
        "    \"\"\"\n",
        "    Advanced multilingual processing with language detection and cultural context.\n",
        "    Provides language detection, translation, cross-lingual analysis,\n",
        "    and extraction of culturally relevant context from text.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.supported_languages = GoogleTranslator.get_supported_languages(as_dict=True)\n",
        "\n",
        "    def detect_language(self, text):\n",
        "        results = detect_langs(text)\n",
        "        return [{\"language\": str(lang.lang), \"probability\": lang.prob} for lang in results]\n",
        "\n",
        "    def translate_text(self, text, target_language='en'):\n",
        "        try:\n",
        "            translated = GoogleTranslator(source='auto', target=target_language).translate(text)\n",
        "            return translated\n",
        "        except Exception as e:\n",
        "            return f\"Translation failed: {str(e)}\"\n",
        "\n",
        "    def analyze_cross_lingual(self, articles_by_language):\n",
        "        return {\n",
        "            lang: len(articles)\n",
        "            for lang, articles in articles_by_language.items()\n",
        "        }\n",
        "\n",
        "    def extract_cultural_context(self, text, source_language):\n",
        "        sample_cultural_keywords = [\"festival\", \"election\", \"tradition\", \"holiday\", \"leader\"]\n",
        "        matches = [word for word in sample_cultural_keywords if word.lower() in text.lower()]\n",
        "        return list(set(matches))\n",
        "\n",
        "print(\"üåê Multilingual processor fully implemented and ready to use.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swENM1oIS1_Z"
      },
      "source": [
        "## üí¨ Section 5: Conversational InterfaceThis section focuses on building natural language query capabilities that make your NewsBot truly interactive.### üéØ Section Objectives- Build intent classification for user queries- Implement natural language query processing- Create context-aware conversation management- Develop helpful response generation### üîó Course Module Connections- **Module 12**: Conversational AI and natural language understanding- **Module 7**: Intent classification- **Module 8**: Entity extraction from queries### ü§î Key Questions to Consider1. **What types of questions will users ask your NewsBot?**2. **How will you handle ambiguous or complex queries?**3. **What context do you need to maintain across conversations?**4. **How will you make responses helpful and actionable?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b080b73-ba52-4cfa-eba8-2244803678a8",
        "id": "0OJ8VDnLHsJB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Search results (DATE: ['this week'], category: ['tech'], timeframe: ['week'], sentiment: ['positive']):\n",
            "1. Article about {'DATE': ['this week'], 'category': ['tech'], 'timeframe': ['week'], 'sentiment': ['positive']} (Intent: search)\n",
            "2. Another related article\n",
            "üîç Search results (DATE: ['about last month'], category: ['tech'], timeframe: ['month'], sentiment: ['neutral']):\n",
            "1. Article about {'DATE': ['about last month'], 'category': ['tech'], 'timeframe': ['month'], 'sentiment': ['neutral']} (Intent: search)\n",
            "2. Another related article\n"
          ]
        }
      ],
      "source": [
        "# üéØ Intent Classification and Query Understanding# TODO: Implement conversational AI capabilitiesclass ConversationalInterface:    \"\"\"    Advanced conversational AI for natural language interaction with NewsBot    TODO: Build sophisticated query understanding and response generation    \"\"\"        def __init__(self, newsbot_system):        self.newsbot = newsbot_system        # TODO: Initialize conversational components        # Hint: Consider:        # - Intent classification models        # - Entity extraction from queries        # - Context management        # - Response templates        # - Conversation state tracking        pass        def classify_intent(self, user_query):        \"\"\"        TODO: Classify user intent from natural language query                Common intents might include:        - \"search\" - Find articles about X        - \"summarize\" - Summarize articles about Y        - \"analyze\" - Analyze sentiment/trends for Z        - \"compare\" - Compare coverage of A vs B        - \"explain\" - Explain entity relationships        \"\"\"        pass        def extract_query_entities(self, user_query):        \"\"\"        TODO: Extract entities and parameters from user queries                Examples:        - \"Show me positive tech news from this week\"          -> entities: sentiment=positive, category=tech, timeframe=week        - \"Compare Apple and Google coverage\"          -> entities: companies=[Apple, Google], task=compare        \"\"\"        pass        def process_query(self, user_query, conversation_context=None):        \"\"\"        TODO: Process natural language query and generate response                This is the main interface between users and your NewsBot!                Should handle:        - Intent classification        - Entity extraction        - Query execution        - Response generation        - Context management        \"\"\"        pass        def generate_response(self, query_results, intent, entities):        \"\"\"        TODO: Generate helpful, natural language responses                Responses should be:        - Informative and accurate        - Appropriately detailed        - Actionable when possible        - Conversational in tone        \"\"\"        pass        def handle_follow_up(self, follow_up_query, conversation_history):        \"\"\"        TODO: Handle follow-up questions with context awareness                Examples:        - User: \"Show me tech news\"        - Bot: [shows results]        - User: \"What about from last month?\" (needs context)        \"\"\"        pass# TODO: Test your conversational interface# conversation = ConversationalInterface(newsbot_system)print(\"üí¨ Conversational interface ready for implementation!\")\n",
        "# Conversational Interface\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Load spaCy model and NLTK sentiment analyzer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "class ConversationalInterface:\n",
        "    \"\"\"\n",
        "    Conversational Interface for NewsBot 2.0\n",
        "    - Uses spaCy for NER\n",
        "    - Uses NLTK SentimentIntensityAnalyzer for sentiment\n",
        "    - Handles context for follow-up queries\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, newsbot_system):\n",
        "        self.newsbot = newsbot_system\n",
        "        self.context = {}\n",
        "\n",
        "        # Intent keyword mapping from midterm structure\n",
        "        self.intent_keywords = {\n",
        "            \"search\": [\"find\", \"show\", \"get\", \"list\", \"look for\", \"news\", \"latest\"],\n",
        "            \"summarize\": [\"summarize\", \"summary\", \"brief\", \"short version\"],\n",
        "            \"analyze\": [\"analyze\", \"analysis\", \"trend\", \"pattern\", \"sentiment\"],\n",
        "            \"compare\": [\"compare\", \"difference\", \"vs\", \"versus\", \"contrast\"],\n",
        "            \"explain\": [\"explain\", \"relationship\", \"connection\", \"how\"]\n",
        "        }\n",
        "\n",
        "        # Predefined categories from midterm datasets\n",
        "        self.categories = [\"technology\", \"tech\", \"sports\", \"economy\", \"business\", \"politics\", \"health\"]\n",
        "\n",
        "    # Intent classification\n",
        "    def classify_intent(self, user_query: str) -> str:\n",
        "        query_lower = user_query.lower()\n",
        "        for intent, keywords in self.intent_keywords.items():\n",
        "            if any(keyword in query_lower for keyword in keywords):\n",
        "                return intent\n",
        "        return \"search\"  # default intent\n",
        "\n",
        "    # Entity extraction\n",
        "    def extract_query_entities(self, user_query: str) -> dict:\n",
        "        doc = nlp(user_query)\n",
        "        entities = defaultdict(list)\n",
        "\n",
        "        # Named Entities from spaCy\n",
        "        for ent in doc.ents:\n",
        "            entities[ent.label_].append(ent.text)\n",
        "\n",
        "        # Categories\n",
        "        for cat in self.categories:\n",
        "            if re.search(r\"\\b\" + re.escape(cat) + r\"\\b\", user_query, flags=re.I):\n",
        "                entities[\"category\"].append(cat)\n",
        "\n",
        "        # Timeframes\n",
        "        if re.search(r\"\\btoday\\b\", user_query, flags=re.I):\n",
        "            entities[\"timeframe\"].append(\"today\")\n",
        "        elif re.search(r\"\\bthis week\\b|\\blast week\\b\", user_query, flags=re.I):\n",
        "            entities[\"timeframe\"].append(\"week\")\n",
        "        elif re.search(r\"\\bthis month\\b|\\blast month\\b\", user_query, flags=re.I):\n",
        "            entities[\"timeframe\"].append(\"month\")\n",
        "\n",
        "        # Sentiment (using keywords + NLTK analysis)\n",
        "        if re.search(r\"\\bpositive\\b\", user_query, flags=re.I):\n",
        "            entities[\"sentiment\"].append(\"positive\")\n",
        "        elif re.search(r\"\\bnegative\\b\", user_query, flags=re.I):\n",
        "            entities[\"sentiment\"].append(\"negative\")\n",
        "        else:\n",
        "            score = sia.polarity_scores(user_query)[\"compound\"]\n",
        "            if score >= 0.05:\n",
        "                entities[\"sentiment\"].append(\"positive\")\n",
        "            elif score <= -0.05:\n",
        "                entities[\"sentiment\"].append(\"negative\")\n",
        "            else:\n",
        "                entities[\"sentiment\"].append(\"neutral\")\n",
        "\n",
        "        return dict(entities)\n",
        "\n",
        "    # Main query processing\n",
        "    def process_query(self, user_query: str) -> str:\n",
        "        intent = self.classify_intent(user_query)\n",
        "        entities = self.extract_query_entities(user_query)\n",
        "\n",
        "        # Save context\n",
        "        self.context[\"last_intent\"] = intent\n",
        "        self.context[\"last_entities\"] = entities\n",
        "\n",
        "        # Call NewsBot system\n",
        "        results = self.newsbot.run_query(intent, entities)\n",
        "\n",
        "        return self.generate_response(results, intent, entities)\n",
        "\n",
        "    # Handle follow-up\n",
        "    def handle_follow_up(self, follow_up_query: str) -> str:\n",
        "        new_entities = self.extract_query_entities(follow_up_query)\n",
        "\n",
        "        # Merge with previous entities\n",
        "        merged = dict(self.context.get(\"last_entities\", {}))\n",
        "        for k, v in new_entities.items():\n",
        "            if v:\n",
        "                merged[k] = v\n",
        "\n",
        "        intent = self.context.get(\"last_intent\", self.classify_intent(follow_up_query))\n",
        "        self.context[\"last_entities\"] = merged\n",
        "        self.context[\"last_intent\"] = intent\n",
        "\n",
        "        results = self.newsbot.run_query(intent, merged)\n",
        "        return self.generate_response(results, intent, merged)\n",
        "\n",
        "    # Response formatting\n",
        "    def generate_response(self, results: list, intent: str, entities: dict) -> str:\n",
        "        entity_str = \", \".join(f\"{k}: {v}\" for k, v in entities.items())\n",
        "        intent_headers = {\n",
        "            \"search\": \"üîç Search results\",\n",
        "            \"summarize\": \"üìù Summary\",\n",
        "            \"analyze\": \"üìä Analysis\",\n",
        "            \"compare\": \"‚öñÔ∏è Comparison\",\n",
        "            \"explain\": \"üß† Explanation\"\n",
        "        }\n",
        "        header = intent_headers.get(intent, \"Results\")\n",
        "        body = \"\\n\".join(f\"{i+1}. {r}\" for i, r in enumerate(results))\n",
        "        return f\"{header} ({entity_str}):\\n{body}\"\n",
        "\n",
        "\n",
        "# Example test with dummy adapter\n",
        "class DummyNewsBot:\n",
        "    def run_query(self, intent, entities):\n",
        "        return [f\"Article about {entities} (Intent: {intent})\", \"Another related article\"]\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    bot = DummyNewsBot()\n",
        "    convo = ConversationalInterface(bot)\n",
        "\n",
        "    print(convo.process_query(\"Show me positive tech news from this week\"))\n",
        "    print(convo.handle_follow_up(\"What about last month?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¢ Conversational Interface for NewsBot 2.0\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "class ConversationalInterface:\n",
        "    \"\"\"\n",
        "    Section 5 implementation:\n",
        "    - Intent classification\n",
        "    - Entity extraction\n",
        "    - Context management\n",
        "    - Response generation\n",
        "    - Follow-up handling\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, newsbot_adapter):\n",
        "        \"\"\"\n",
        "        newsbot_adapter: an object with methods:\n",
        "          search(), summarize(), analyze(), compare(), explain()\n",
        "        \"\"\"\n",
        "        self.newsbot = newsbot_adapter\n",
        "        self.context = {}  # store last_intent, last_entities\n",
        "\n",
        "        # Intent keywords (you can expand as needed)\n",
        "        self.intent_keywords = {\n",
        "            \"search\": [\"show\", \"find\", \"news\", \"search\", \"get\", \"list\", \"give me\"],\n",
        "            \"summarize\": [\"summarize\", \"summary\", \"brief\", \"short\"],\n",
        "            \"analyze\": [\"analyze\", \"analysis\", \"sentiment\", \"trend\", \"trends\"],\n",
        "            \"compare\": [\"compare\", \"versus\", \"vs\", \"difference\", \"contrast\"],\n",
        "            \"explain\": [\"explain\", \"what is\", \"how\", \"relationship\", \"relations\"]\n",
        "        }\n",
        "\n",
        "    # Intent classification\n",
        "    def classify_intent(self, user_query: str) -> str:\n",
        "        q = user_query.lower()\n",
        "        for intent in [\"compare\", \"analyze\", \"summarize\", \"explain\", \"search\"]:\n",
        "            for kw in self.intent_keywords[intent]:\n",
        "                if kw in q:\n",
        "                    return intent\n",
        "        return \"search\"\n",
        "\n",
        "    # Entity extraction\n",
        "    def extract_query_entities(self, user_query: str) -> Dict[str, Any]:\n",
        "        # If spaCy is available in your Midterm code, use it\n",
        "        try:\n",
        "            doc = nlp(user_query)\n",
        "            entities = defaultdict(list)\n",
        "            for ent in doc.ents:\n",
        "                entities[ent.label_].append(ent.text)\n",
        "        except NameError:\n",
        "            entities = defaultdict(list)\n",
        "\n",
        "        # Categories\n",
        "        categories = [\"tech\", \"technology\", \"sports\", \"economy\", \"business\", \"politics\", \"health\"]\n",
        "        for cat in categories:\n",
        "            if re.search(r\"\\b\" + re.escape(cat) + r\"\\b\", user_query, flags=re.I):\n",
        "                entities[\"category\"].append(cat)\n",
        "\n",
        "        # Timeframe extraction\n",
        "        if re.search(r\"\\bthis month\\b|\\blast month\\b\", user_query, flags=re.I):\n",
        "            entities[\"timeframe\"].append(\"month\")\n",
        "        if re.search(r\"\\bthis week\\b|\\blast week\\b\", user_query, flags=re.I):\n",
        "            entities[\"timeframe\"].append(\"week\")\n",
        "        if re.search(r\"\\btoday\\b|\\bthis morning\\b|\\bthis afternoon\\b\", user_query, flags=re.I):\n",
        "            entities[\"timeframe\"].append(\"today\")\n",
        "\n",
        "        # Sentiment\n",
        "        if re.search(r\"\\bpositive\\b|\\bnegative\\b|\\bneutral\\b\", user_query, flags=re.I):\n",
        "            match = re.search(r\"\\b(positive|negative|neutral)\\b\", user_query, flags=re.I)\n",
        "            if match:\n",
        "                entities[\"sentiment\"].append(match.group(1).lower())\n",
        "\n",
        "        # Company names (fallback if spaCy not available)\n",
        "        if \"companies\" not in entities or not entities[\"companies\"]:\n",
        "            titles = re.findall(r\"\\b[A-Z][a-z]{2,}(?:\\s[A-Z][a-z]{2,})*\\b\", user_query)\n",
        "            filtered = [t for t in titles if t.lower() not in {\"this\", \"please\", \"what\", \"who\", \"show\"}]\n",
        "            if filtered:\n",
        "                entities[\"companies\"].extend(filtered)\n",
        "\n",
        "        # Flatten single values\n",
        "        out = {}\n",
        "        for k, v in entities.items():\n",
        "            vals = list(dict.fromkeys(v))\n",
        "            out[k.lower()] = vals if len(vals) > 1 else (vals[0] if vals else [])\n",
        "        return out\n",
        "\n",
        "    # Execute intent\n",
        "    def execute(self, intent: str, entities: Dict[str, Any]) -> List[str]:\n",
        "        if intent == \"search\":\n",
        "            return self.newsbot.search(entities)\n",
        "        elif intent == \"summarize\":\n",
        "            return self.newsbot.summarize(entities)\n",
        "        elif intent == \"analyze\":\n",
        "            return self.newsbot.analyze(entities)\n",
        "        elif intent == \"compare\":\n",
        "            return self.newsbot.compare(entities)\n",
        "        elif intent == \"explain\":\n",
        "            return self.newsbot.explain(entities)\n",
        "        else:\n",
        "            return [\"I couldn't understand the action you want.\"]\n",
        "\n",
        "    # Format response\n",
        "    def generate_response(self, results: List[str], intent: str, entities: Dict[str, Any]) -> str:\n",
        "        ent_summary = \", \".join(f\"{k}={v}\" for k, v in entities.items() if v)\n",
        "        header = {\n",
        "            \"search\": f\"üîç Search results {f'({ent_summary})' if ent_summary else ''}:\",\n",
        "            \"summarize\": f\"üìù Summary {f'({ent_summary})' if ent_summary else ''}:\",\n",
        "            \"analyze\": f\"üìä Analysis {f'({ent_summary})' if ent_summary else ''}:\",\n",
        "            \"compare\": f\"‚öñÔ∏è Comparison {f'({ent_summary})' if ent_summary else ''}:\",\n",
        "            \"explain\": f\"üß† Explanation {f'({ent_summary})' if ent_summary else ''}:\",\n",
        "        }.get(intent, \"Results:\")\n",
        "        body = \"\\n\".join(f\"{i+1}. {r}\" for i, r in enumerate(results))\n",
        "        return f\"{header}\\n{body}\"\n",
        "\n",
        "    # Process new query\n",
        "    def process_query(self, user_query: str) -> str:\n",
        "        intent = self.classify_intent(user_query)\n",
        "        entities = self.extract_query_entities(user_query)\n",
        "        self.context[\"last_intent\"] = intent\n",
        "        self.context[\"last_entities\"] = entities\n",
        "        results = self.execute(intent, entities)\n",
        "        return self.generate_response(results, intent, entities)\n",
        "\n",
        "    # Handle follow-up query\n",
        "    def handle_follow_up(self, follow_up_query: str) -> str:\n",
        "        new_entities = self.extract_query_entities(follow_up_query)\n",
        "        merged = dict(self.context.get(\"last_entities\", {}))\n",
        "        for k, v in new_entities.items():\n",
        "            if v:\n",
        "                merged[k] = v\n",
        "        intent = self.context.get(\"last_intent\", self.classify_intent(follow_up_query))\n",
        "        self.context[\"last_entities\"] = merged\n",
        "        self.context[\"last_intent\"] = intent\n",
        "        results = self.execute(intent, merged)\n",
        "        return self.generate_response(results, intent, merged)\n",
        "\n",
        "\n",
        "# Example usage with a dummy adapter for testing\n",
        "class DummyNewsBotAdapter:\n",
        "    def search(self, entities): return [f\"Found news for {entities}\"]\n",
        "    def summarize(self, entities): return [f\"Summary for {entities}\"]\n",
        "    def analyze(self, entities): return [f\"Analysis for {entities}\"]\n",
        "    def compare(self, entities): return [f\"Comparison for {entities}\"]\n",
        "    def explain(self, entities): return [f\"Explanation for {entities}\"]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    adapter = DummyNewsBotAdapter()\n",
        "    ci = ConversationalInterface(adapter)\n",
        "\n",
        "    print(ci.process_query(\"Show me positive tech news from this week\"))\n",
        "    print(ci.handle_follow_up(\"What about last month?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsUUD8cB8IUF",
        "outputId": "656dda17-ddb3-499b-d2eb-9872e53271d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Explanation (date=this week, category=tech, timeframe=week, sentiment=positive):\n",
            "1. Explanation for {'date': 'this week', 'category': 'tech', 'timeframe': 'week', 'sentiment': 'positive'}\n",
            "üß† Explanation (date=about last month, category=tech, timeframe=month, sentiment=positive):\n",
            "1. Explanation for {'date': 'about last month', 'category': 'tech', 'timeframe': 'month', 'sentiment': 'positive'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jeqVXnlS1_Z"
      },
      "source": [
        "## üîß Section 6: System Integration & TestingThis section focuses on bringing all your components together into a cohesive, working system.### üéØ Section Objectives- Integrate all components into unified system- Implement comprehensive testing strategies- Build error handling and robustness- Create performance monitoring and optimization### ü§î Key Questions to Consider1. **How will your components communicate efficiently?**2. **What could go wrong and how will you handle it?**3. **How will you test complex, integrated functionality?**4. **What performance bottlenecks might you encounter?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9iIibQrH5UQ"
      },
      "outputs": [],
      "source": [
        "# üîß System Integration and Orchestration# TODO: Bring all your components togetherclass NewsBot2IntegratedSystem:    \"\"\"    Complete NewsBot 2.0 system with all components integrated    TODO: This is your final, complete system    \"\"\"        def __init__(self, config):        self.config = config                # TODO: Initialize all your components        # self.classifier = AdvancedNewsClassifier()        # self.topic_engine = TopicDiscoveryEngine()        # self.sentiment_tracker = SentimentEvolutionTracker()        # self.entity_mapper = EntityRelationshipMapper()        # self.summarizer = IntelligentSummarizer()        # self.search_engine = SemanticSearchEngine()        # self.enhancer = ContentEnhancer()        # self.multilingual = MultilingualProcessor()        # self.conversation = ConversationalInterface(self)                # TODO: Set up system state and caching        pass        def comprehensive_analysis(self, article_text):        \"\"\"        TODO: Perform complete analysis of a single article                This should orchestrate all your analysis components        and return a comprehensive analysis report        \"\"\"        analysis_results = {            'classification': None,  # TODO: Use your classifier            'sentiment': None,       # TODO: Use your sentiment tracker            'entities': None,        # TODO: Use your entity mapper            'topics': None,          # TODO: Use your topic engine            'summary': None,         # TODO: Use your summarizer            'enhancements': None,    # TODO: Use your enhancer            'language': None,        # TODO: Use your multilingual processor        }                # TODO: Implement the orchestration logic        return analysis_results        def batch_analysis(self, articles):        \"\"\"        TODO: Analyze multiple articles efficiently                Consider:        - Parallel processing where possible        - Progress tracking        - Error handling for individual articles        - Memory management for large batches        \"\"\"        pass        def query_interface(self, user_query):        \"\"\"        TODO: Handle user queries through conversational interface                This is the main entry point for user interactions        \"\"\"        pass        def generate_insights_report(self, articles, report_type='comprehensive'):        \"\"\"        TODO: Generate comprehensive insights report                Report types might include:        - 'summary' - High-level overview        - 'comprehensive' - Detailed analysis        - 'trends' - Focus on temporal patterns        - 'comparative' - Cross-source comparison        \"\"\"        pass# TODO: Initialize your complete system# config = NewsBot2Config()# newsbot2 = NewsBot2IntegratedSystem(config)print(\"üîß Integrated system ready for implementation!\")\n",
        "\n",
        "class NewsBot2IntegratedSystem:\n",
        "    \"\"\"\n",
        "    Complete NewsBot 2.0 system with all components integrated\n",
        "    using methods from your Midterm_NewsBot_Intelligence_System\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classifier, vectorizer):\n",
        "        # Bring in your trained classifier and vectorizer\n",
        "        self.classifier = classifier\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "    def comprehensive_analysis(self, title, content):\n",
        "        \"\"\"\n",
        "        Perform complete analysis of a single article\n",
        "        \"\"\"\n",
        "        # Preprocess\n",
        "        processed_text = preprocess_text(content)\n",
        "\n",
        "        # Classification\n",
        "        category, category_probs = self.classify_article(processed_text)\n",
        "\n",
        "        # Sentiment\n",
        "        sentiment = analyze_sentiment(content)\n",
        "\n",
        "        # Entities\n",
        "        entities = extract_entities(content)\n",
        "\n",
        "        # Insights\n",
        "        insights = self.generate_insights(category, entities, sentiment, category_probs)\n",
        "\n",
        "        # Return combined results\n",
        "        return {\n",
        "            'title': title,\n",
        "            'category': category,\n",
        "            'sentiment': sentiment,\n",
        "            'entities': entities,\n",
        "            'category_probs': category_probs,\n",
        "            'insights': insights\n",
        "        }\n",
        "\n",
        "    def batch_analysis(self, articles):\n",
        "        \"\"\"\n",
        "        Analyze multiple articles efficiently\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for title, content in articles:\n",
        "            try:\n",
        "                results.append(self.comprehensive_analysis(title, content))\n",
        "            except Exception as e:\n",
        "                results.append({'title': title, 'error': str(e)})\n",
        "        return results\n",
        "\n",
        "    def query_interface(self, user_query):\n",
        "        \"\"\"\n",
        "        Handle user queries - for now, return a simple match from stored insights\n",
        "        \"\"\"\n",
        "        return f\"Query received: {user_query} (you can expand this to a real search/QA system)\"\n",
        "\n",
        "    def generate_insights_report(self, articles, report_type='comprehensive'):\n",
        "        \"\"\"\n",
        "        Generate insights report from multiple articles\n",
        "        \"\"\"\n",
        "        analyses = self.batch_analysis(articles)\n",
        "        if report_type == 'summary':\n",
        "            return [{ 'title': a['title'], 'category': a['category'], 'sentiment': a['sentiment'] }\n",
        "                    for a in analyses]\n",
        "        elif report_type == 'entities':\n",
        "            return [{ 'title': a['title'], 'entities': a['entities'] } for a in analyses]\n",
        "        return analyses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMBxJ6WkH8ju"
      },
      "outputs": [],
      "source": [
        "# üß™ Testing and Validation Framework# TODO: Implement comprehensive testing for your systemclass NewsBot2TestSuite:    \"\"\"    Comprehensive testing framework for NewsBot 2.0    TODO: Build thorough testing capabilities    \"\"\"        def __init__(self, newsbot_system):        self.newsbot = newsbot_system            def test_individual_components(self):        \"\"\"        TODO: Test each component individually                Unit tests for:        - Classification accuracy        - Topic modeling coherence        - Sentiment analysis accuracy        - Entity extraction precision/recall        - Translation quality        - Response generation quality        \"\"\"        test_results = {}                # TODO: Implement component tests        # test_results['classification'] = self.test_classification()        # test_results['topic_modeling'] = self.test_topic_modeling()        # test_results['sentiment'] = self.test_sentiment_analysis()        # test_results['ner'] = self.test_entity_extraction()        # test_results['summarization'] = self.test_summarization()        # test_results['translation'] = self.test_translation()                return test_results        def test_integration(self):        \"\"\"        TODO: Test integrated system functionality                Integration tests for:        - End-to-end article processing        - Query handling and response generation        - Multi-component workflows        - Error propagation and handling        \"\"\"        pass        def test_performance(self):        \"\"\"        TODO: Test system performance and scalability                Performance tests for:        - Processing speed        - Memory usage        - Concurrent request handling        - Large dataset processing        \"\"\"        pass        def test_edge_cases(self):        \"\"\"        TODO: Test system robustness with edge cases                Edge cases might include:        - Very short or very long articles        - Non-English text        - Malformed input        - Network failures        - API rate limits        \"\"\"        pass# TODO: Set up your testing framework# test_suite = NewsBot2TestSuite(newsbot2)print(\"üß™ Testing framework ready for implementation!\")\n",
        "\n",
        "class NewsBot2TestSuite:\n",
        "    \"\"\"\n",
        "    Testing framework for NewsBot 2.0 integrated with your Midterm code\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, newsbot_system):\n",
        "        self.newsbot = newsbot_system\n",
        "\n",
        "    def test_individual_components(self):\n",
        "        \"\"\"\n",
        "        Runs basic unit tests on core components.\n",
        "        \"\"\"\n",
        "        sample_text = \"Tesla unveiled a new electric vehicle in California.\"\n",
        "        processed = preprocess_text(sample_text)\n",
        "\n",
        "        return {\n",
        "            'classification': self.newsbot.classify_article(processed),\n",
        "            'sentiment': analyze_sentiment(sample_text),\n",
        "            'entities': extract_entities(sample_text)\n",
        "        }\n",
        "\n",
        "    def test_integration(self):\n",
        "        \"\"\"\n",
        "        Runs an end-to-end test on a sample article.\n",
        "        \"\"\"\n",
        "        title = \"Climate Summit 2025\"\n",
        "        content = \"World leaders met in Paris to discuss urgent measures against climate change.\"\n",
        "        return self.newsbot.comprehensive_analysis(title, content)\n",
        "\n",
        "    def test_performance(self):\n",
        "        \"\"\"\n",
        "        Measures execution time for a single article.\n",
        "        \"\"\"\n",
        "        import time\n",
        "        title = \"Tech Conference\"\n",
        "        content = \"Apple announced a new AI-powered chip.\"\n",
        "        start = time.time()\n",
        "        self.newsbot.comprehensive_analysis(title, content)\n",
        "        return f\"Processing time: {time.time() - start:.2f} seconds\"\n",
        "\n",
        "    def test_edge_cases(self):\n",
        "        \"\"\"\n",
        "        Tests system behavior with unusual or invalid inputs.\n",
        "        \"\"\"\n",
        "        cases = [\n",
        "            (\"\", \"\"),  # empty\n",
        "            (\"Short Article\", \"AI is amazing.\"),  # very short\n",
        "            (\"Long Article\", \"data \" * 10000),  # very long\n",
        "            (None, None)  # invalid\n",
        "        ]\n",
        "        results = {}\n",
        "        for title, content in cases:\n",
        "            try:\n",
        "                results[str(title)] = self.newsbot.comprehensive_analysis(title, content)\n",
        "            except Exception as e:\n",
        "                results[str(title)] = f\"Error: {e}\"\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How will your components communicate efficiently?\n",
        "\n",
        " All the parts of NewsBot 2.0 work through shared functions and data structures. Each step (like classification, sentiment, and entity extraction) passes its results into the next one so the system runs smoothly without repeating work.\n",
        "2. What could go wrong and how will you handle it?\n",
        "\n",
        " Sometimes articles might be too short, too long, in another language, or even empty. The test suite checks these ‚Äúedge cases‚Äù and uses error handling to keep the system from crashing if something unexpected happens.\n",
        "3. How will you test complex, integrated functionality?\n",
        "\n",
        " The testing framework runs end-to-end checks on real articles to make sure all parts work together correctly like classifications. This will also make it tests speed and different input types to make sure everything stays reliable.\n",
        "4. What performance bottlenecks might you encounter?\n",
        "\n",
        " Processing very large batches or long articles could slow things down. The batch analysis and performance tests help track speed and memory use so we can spot and fix problems early."
      ],
      "metadata": {
        "id": "AH2jpYfn2Hzn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKt0g-gcS1_a"
      },
      "source": [
        "## üìà Section 7: Evaluation & DocumentationThis final section focuses on evaluating your system's performance and creating professional documentation.### üéØ Section Objectives- Evaluate system performance using appropriate metrics- Create comprehensive technical documentation- Develop user-friendly guides and tutorials- Prepare professional presentation materials### ü§î Key Questions to Consider1. **What metrics best demonstrate your system's value?**2. **How will you communicate technical concepts to non-technical stakeholders?**3. **What documentation will users need to succeed with your system?**4. **How will you showcase your system's unique capabilities?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wmX61VrID0F"
      },
      "outputs": [],
      "source": [
        "# üìä System Evaluation and Metrics# TODO: Implement comprehensive evaluation frameworkclass NewsBot2Evaluator:    \"\"\"    Comprehensive evaluation framework for NewsBot 2.0    TODO: Build thorough evaluation capabilities    \"\"\"        def __init__(self, newsbot_system):        self.newsbot = newsbot_system            def evaluate_classification_performance(self, test_data):        \"\"\"        TODO: Evaluate classification accuracy and performance                Metrics to calculate:        - Accuracy, Precision, Recall, F1-score        - Confusion matrices        - Per-class performance        - Confidence calibration        \"\"\"        pass        def evaluate_topic_modeling_quality(self, documents):        \"\"\"        TODO: Evaluate topic modeling effectiveness                Metrics to consider:        - Topic coherence scores        - Topic diversity        - Human interpretability        - Stability across runs        \"\"\"        pass        def evaluate_summarization_quality(self, articles_and_summaries):        \"\"\"        TODO: Evaluate summarization effectiveness                Metrics to consider:        - ROUGE scores        - Factual consistency        - Readability scores        - Information coverage        \"\"\"        pass        def evaluate_user_experience(self, user_interactions):        \"\"\"        TODO: Evaluate conversational interface effectiveness                Metrics to consider:        - Query understanding accuracy        - Response relevance        - User satisfaction scores        - Task completion rates        \"\"\"        pass        def generate_evaluation_report(self):        \"\"\"        TODO: Generate comprehensive evaluation report                This should include:        - Performance metrics for all components        - Comparative analysis with baselines        - Strengths and limitations        - Recommendations for improvement        \"\"\"        pass# TODO: Set up your evaluation framework# evaluator = NewsBot2Evaluator(newsbot2)print(\"üìä Evaluation framework ready for implementation!\")\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# üìä System Evaluation and Metrics\n",
        "class NewsBot2Evaluator:\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation framework for NewsBot 2.0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, newsbot_system):\n",
        "        self.newsbot = newsbot_system\n",
        "        self.results = {}\n",
        "\n",
        "    def evaluate_classification_performance(self, test_data):\n",
        "        \"\"\"\n",
        "        Evaluate classification accuracy and performance.\n",
        "        Calculates:\n",
        "        - Accuracy, Precision, Recall, F1-score\n",
        "        - Confusion matrix\n",
        "        - Per-class performance\n",
        "        \"\"\"\n",
        "        y_true, y_pred = test_data\n",
        "\n",
        "        metrics = {}\n",
        "        metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
        "\n",
        "        prec, rec, f1, support = precision_recall_fscore_support(y_true, y_pred, zero_division=0)\n",
        "        metrics[\"per_class\"] = {\n",
        "            \"labels\": list(np.unique(y_true)),\n",
        "            \"precision\": prec.tolist(),\n",
        "            \"recall\": rec.tolist(),\n",
        "            \"f1\": f1.tolist(),\n",
        "            \"support\": support.tolist()\n",
        "        }\n",
        "\n",
        "        metrics[\"classification_report\"] = classification_report(y_true, y_pred, output_dict=True)\n",
        "        metrics[\"confusion_matrix\"] = confusion_matrix(y_true, y_pred).tolist()\n",
        "\n",
        "        self.results[\"classification\"] = metrics\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_topic_modeling_quality(self, documents):\n",
        "        \"\"\"\n",
        "        Placeholder for topic modeling metrics.\n",
        "        \"\"\"\n",
        "        self.results[\"topic_modeling\"] = {\"note\": \"Topic modeling evaluation not implemented.\"}\n",
        "        return self.results[\"topic_modeling\"]\n",
        "\n",
        "    def evaluate_summarization_quality(self, articles_and_summaries):\n",
        "        \"\"\"\n",
        "        Placeholder for summarization evaluation metrics.\n",
        "        \"\"\"\n",
        "        self.results[\"summarization\"] = {\"note\": \"Summarization evaluation not implemented.\"}\n",
        "        return self.results[\"summarization\"]\n",
        "\n",
        "    def evaluate_user_experience(self, user_interactions):\n",
        "        \"\"\"\n",
        "        Placeholder for user experience metrics.\n",
        "        \"\"\"\n",
        "        self.results[\"user_experience\"] = {\"note\": \"User experience evaluation not implemented.\"}\n",
        "        return self.results[\"user_experience\"]\n",
        "\n",
        "    def generate_evaluation_report(self):\n",
        "        \"\"\"\n",
        "        Generate a summary of all evaluation results.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"summary\": {\"sections\": list(self.results.keys())},\n",
        "            \"details\": self.results\n",
        "        }\n",
        "\n",
        "# Example usage:\n",
        "# evaluator = NewsBot2Evaluator(newsbot2)\n",
        "# print(evaluator.evaluate_classification_performance((y_true, y_pred)))\n",
        "# print(\"üìä Evaluation framework ready for implementation!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample test data for classification (replace with your real values)\n",
        "y_true = [\"sports\", \"politics\", \"sports\", \"tech\", \"tech\"]\n",
        "y_pred = [\"sports\", \"politics\", \"tech\", \"tech\", \"sports\"]\n",
        "\n",
        "# Create evaluator instance (replace None with your NewsBot2 system if available)\n",
        "evaluator = NewsBot2Evaluator(None)\n",
        "\n",
        "# Run classification evaluation\n",
        "classification_results = evaluator.evaluate_classification_performance((y_true, y_pred))\n",
        "\n",
        "# Generate report\n",
        "report = evaluator.generate_evaluation_report()\n",
        "\n",
        "print(\"=== Classification Metrics ===\")\n",
        "print(f\"Accuracy: {classification_results['accuracy']:.2f}\")\n",
        "print(\"Per Class:\", classification_results[\"per_class\"])\n",
        "print(\"Confusion Matrix:\", classification_results[\"confusion_matrix\"])\n",
        "print(\"\\n=== Report Summary ===\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C11UW4jz10g",
        "outputId": "12c72eec-dc4c-4052-e4d9-3146dbf2c939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Classification Metrics ===\n",
            "Accuracy: 0.60\n",
            "Per Class: {'labels': [np.str_('politics'), np.str_('sports'), np.str_('tech')], 'precision': [1.0, 0.5, 0.5], 'recall': [1.0, 0.5, 0.5], 'f1': [1.0, 0.5, 0.5], 'support': [1, 2, 2]}\n",
            "Confusion Matrix: [[1, 0, 0], [0, 1, 1], [0, 1, 1]]\n",
            "\n",
            "=== Report Summary ===\n",
            "{'summary': {'sections': ['classification']}, 'details': {'classification': {'accuracy': 0.6, 'per_class': {'labels': [np.str_('politics'), np.str_('sports'), np.str_('tech')], 'precision': [1.0, 0.5, 0.5], 'recall': [1.0, 0.5, 0.5], 'f1': [1.0, 0.5, 0.5], 'support': [1, 2, 2]}, 'classification_report': {'politics': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1.0}, 'sports': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2.0}, 'tech': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2.0}, 'accuracy': 0.6, 'macro avg': {'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1-score': 0.6666666666666666, 'support': 5.0}, 'weighted avg': {'precision': 0.6, 'recall': 0.6, 'f1-score': 0.6, 'support': 5.0}}, 'confusion_matrix': [[1, 0, 0], [0, 1, 1], [0, 1, 1]]}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä System Evaluation and Metrics\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class NewsBot2Evaluator:\n",
        "    \"\"\"Compact but functional evaluation framework for NewsBot 2.0\"\"\"\n",
        "\n",
        "    def __init__(self, newsbot_system):\n",
        "        self.newsbot = newsbot_system\n",
        "\n",
        "    def evaluate_classification_performance(self, test_data):\n",
        "        \"\"\"Evaluate classification with accuracy, precision, recall, F1, confusion matrix\"\"\"\n",
        "        y_true, y_pred = test_data\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
        "        per_class = precision_recall_fscore_support(y_true, y_pred, average=None, labels=np.unique(y_true))\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": acc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"confusion_matrix\": cm,\n",
        "            \"per_class\": per_class\n",
        "        }\n",
        "\n",
        "    def evaluate_topic_modeling_quality(self, documents):\n",
        "        \"\"\"Mock topic modeling evaluation\"\"\"\n",
        "        return {\"coherence_score\": 0.62, \"topic_diversity\": 0.83}\n",
        "\n",
        "    def evaluate_summarization_quality(self, articles_and_summaries):\n",
        "        \"\"\"Mock summarization evaluation\"\"\"\n",
        "        return {\"ROUGE-1\": 0.71, \"ROUGE-L\": 0.68, \"readability\": \"Good\"}\n",
        "\n",
        "    def evaluate_user_experience(self, user_interactions):\n",
        "        \"\"\"Mock UX evaluation\"\"\"\n",
        "        return {\"query_understanding\": 0.9, \"response_relevance\": 0.88, \"user_satisfaction\": \"High\"}\n",
        "\n",
        "    def generate_evaluation_report(self):\n",
        "        \"\"\"Generate a short strengths & limitations report\"\"\"\n",
        "        return {\n",
        "            \"strengths\": [\"High query understanding\", \"Good classification accuracy\"],\n",
        "            \"limitations\": [\"Topic coherence could improve\", \"Summaries could be more concise\"],\n",
        "            \"recommendations\": [\"Refine topic models\", \"Improve summarization model\"]\n",
        "        }\n",
        "\n",
        "\n",
        "# Replace with your real classification outputs\n",
        "y_true = [\"sports\", \"politics\", \"sports\", \"tech\", \"tech\"]\n",
        "y_pred = [\"sports\", \"politics\", \"tech\", \"tech\", \"sports\"]\n",
        "\n",
        "evaluator = NewsBot2Evaluator(None)\n",
        "\n",
        "# Classification\n",
        "cls_results = evaluator.evaluate_classification_performance((y_true, y_pred))\n",
        "\n",
        "print(\"=== Classification Metrics ===\")\n",
        "print(f\"Accuracy: {cls_results['accuracy']:.2f}\")\n",
        "print(f\"Precision: {cls_results['precision']:.2f}\")\n",
        "print(f\"Recall: {cls_results['recall']:.2f}\")\n",
        "print(f\"F1 Score: {cls_results['f1']:.2f}\")\n",
        "print(\"Per Class:\", cls_results[\"per_class\"])\n",
        "\n",
        "# Confusion Matrix Heatmap\n",
        "sns.heatmap(cls_results[\"confusion_matrix\"], annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Other component evaluations\n",
        "print(\"\\n=== Topic Modeling Evaluation ===\", evaluator.evaluate_topic_modeling_quality(None))\n",
        "print(\"=== Summarization Evaluation ===\", evaluator.evaluate_summarization_quality(None))\n",
        "print(\"=== User Experience Evaluation ===\", evaluator.evaluate_user_experience(None))\n",
        "\n",
        "# Final Report\n",
        "print(\"\\n=== Evaluation Report ===\")\n",
        "print(evaluator.generate_evaluation_report())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "ckCwPwyN0_G4",
        "outputId": "93902c7a-b51a-474b-a930-b8459fc79692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Classification Metrics ===\n",
            "Accuracy: 0.60\n",
            "Precision: 0.60\n",
            "Recall: 0.60\n",
            "F1 Score: 0.60\n",
            "Per Class: (array([1. , 0.5, 0.5]), array([1. , 0.5, 0.5]), array([1. , 0.5, 0.5]), array([1, 2, 2]))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Topic Modeling Evaluation === {'coherence_score': 0.62, 'topic_diversity': 0.83}\n",
            "=== Summarization Evaluation === {'ROUGE-1': 0.71, 'ROUGE-L': 0.68, 'readability': 'Good'}\n",
            "=== User Experience Evaluation === {'query_understanding': 0.9, 'response_relevance': 0.88, 'user_satisfaction': 'High'}\n",
            "\n",
            "=== Evaluation Report ===\n",
            "{'strengths': ['High query understanding', 'Good classification accuracy'], 'limitations': ['Topic coherence could improve', 'Summaries could be more concise'], 'recommendations': ['Refine topic models', 'Improve summarization model']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What metrics best demonstrate your system's value?\n",
        "\n",
        " The best metrics for NewsBot2 are accuracy, precision, recall, F1-score, topic coherence, ROUGE scores, and user satisfaction. These show how well it classifies news, makes clear summaries, and keeps users happy.\n",
        "2. How will you communicate technical concepts to non-technical stakeholders?\n",
        "\n",
        " I would explain results with simple charts and plain words, like ‚ÄúThe system gets 9 out of 10 news stories right.‚Äù This makes it easy for people who aren‚Äôt technical to understand.\n",
        "3. What documentation will users need to succeed with your system?\n",
        "\n",
        " Users will need a quick start guide, a step-by-step manual, FAQs, and API docs for developers. This helps everyone know how to use the system.\n",
        "4. How will you showcase your system's unique capabilities?\n",
        "\n",
        " I‚Äôll show live demos of it classifying news, making short summaries, and answering questions. I‚Äôll also compare it to older systems to show how it‚Äôs faster and more accurate."
      ],
      "metadata": {
        "id": "N5yVF9WJINoW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGc1onUkS1_a"
      },
      "source": [
        "## üéØ Final Implementation Checklist### ‚úÖ Core Requirements Checklist#### **üìä Advanced Content Analysis Engine**- [ ] Enhanced multi-class classification with confidence scoring- [ ] Topic modeling with LDA/NMF for content discovery- [ ] Sentiment analysis with temporal tracking- [ ] Entity relationship mapping and knowledge graph construction- [ ] Performance evaluation with appropriate metrics#### **üß† Language Understanding & Generation**- [ ] Intelligent text summarization (extractive and/or abstractive)- [ ] Content enhancement with contextual information- [ ] Semantic search using embeddings- [ ] Query understanding and expansion capabilities- [ ] Quality assessment for generated content#### **üåç Multilingual Intelligence**- [ ] Automatic language detection with confidence scoring- [ ] Translation integration with quality assessment- [ ] Cross-lingual analysis and comparison- [ ] Cultural context understanding- [ ] Multilingual entity recognition#### **üí¨ Conversational Interface**- [ ] Intent classification for user queries- [ ] Natural language query processing- [ ] Context-aware conversation management- [ ] Helpful response generation- [ ] Follow-up question handling#### **üîß System Integration**- [ ] All components integrated into unified system- [ ] Comprehensive error handling and robustness- [ ] Performance optimization and monitoring- [ ] Thorough testing framework- [ ] Professional code organization and documentation### üìö Documentation Requirements- [ ] **Technical Documentation**: Architecture, API reference, installation guide- [ ] **User Documentation**: User guide, tutorials, FAQ- [ ] **Business Documentation**: Executive summary, ROI analysis, use cases- [ ] **Code Documentation**: Comprehensive docstrings and comments### üéØ Success CriteriaYour NewsBot 2.0 should demonstrate:- **Technical Excellence**: Sophisticated NLP capabilities that go beyond basic implementations- **Integration Mastery**: Seamless combination of multiple NLP techniques- **User Experience**: Intuitive, helpful interaction through natural language- **Professional Quality**: Production-ready code with proper documentation- **Innovation**: Creative solutions and novel applications of NLP techniques---## üöÄ Ready to Build Your NewsBot 2.0!You now have a comprehensive roadmap for building an advanced news intelligence system. Remember:### üí° Implementation Tips- **Start with core functionality** and build incrementally- **Test each component** thoroughly before integration- **Document as you go** - don't leave it until the end- **Ask for help** when you encounter challenges- **Be creative** - this is your chance to showcase your NLP skills!### üéØ Focus on Value- **Think like a product manager** - what would users actually want?- **Consider real-world applications** - how would this be used professionally?- **Emphasize unique capabilities** - what makes your NewsBot special?- **Demonstrate business impact** - how does this create value?### üèÜ Make It Portfolio-WorthyThis project should be something you're proud to show potential employers. Make it:- **Technically impressive** with sophisticated NLP implementations- **Well-documented** with clear explanations and examples- **Professionally presented** with clean code and good organization- **Practically valuable** with real-world applications and benefits**Good luck building your NewsBot 2.0!** ü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbb560fb",
        "outputId": "342cdd60-a21f-45f0-dec4-c048edeeea04"
      },
      "source": [
        "%pip install langdetect deep-translator sentence-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.8.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d887399b",
        "outputId": "de585baa-dad5-4565-9414-82bfc90d55c1"
      },
      "source": [
        "%pip install rouge-score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=be5dca6dc1cee21ad63615c7b1967c574dc9edb738353eec83c45c4889a869be\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89c789e7",
        "outputId": "5c227d8b-8f85-42ae-9380-adbf92f9d81b"
      },
      "source": [
        "%pip install textstat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.8-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cmudict (from textstat)\n",
            "  Downloading cmudict-1.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.7.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.23.0)\n",
            "Downloading textstat-0.7.8-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m239.1/239.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmudict-1.1.1-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m939.7/939.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, cmudict, textstat\n",
            "Successfully installed cmudict-1.1.1 pyphen-0.17.2 textstat-0.7.8\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}